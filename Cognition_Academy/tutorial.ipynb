{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "SfOXwhlW_Mv1",
      "metadata": {
        "id": "SfOXwhlW_Mv1"
      },
      "source": [
        "# Cognition Academy 2025\n",
        "\n",
        "Léopold Maytié\n",
        "\n",
        "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ruflab/shimmer-tutorials/blob/main/Cognition_Academy/tutorial.ipynb)\n",
        "\n",
        "\n",
        "In this notebook, we will see how to use a model inspired by the Global Workspace Theory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "bc2ad685",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bc2ad685",
        "outputId": "32638fd1-b968-4eba-b715-c85436c4847f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU is available: Tesla T4\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    gpu_name = torch.cuda.get_device_name(0)\n",
        "    print(f\"GPU is available: {gpu_name}\")\n",
        "else:\n",
        "    from IPython.display import HTML\n",
        "    message = \"\"\"\n",
        "    <div style=\"color: red; font-weight: bold; font-size: 16px;\">\n",
        "        GPU is NOT enabled.<br>\n",
        "        Please go to <b>Runtime > Change runtime type > Hardware accelerator > GPU</b> and select \"GPU\".\n",
        "    </div>\n",
        "    \"\"\"\n",
        "    display(HTML(message))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "fa948ba6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fa948ba6",
        "outputId": "b0fa88fd-506c-4939-987e-555b42e7a4e7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.11/dist-packages (2.18.0)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (1.73.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (3.8.2)\n",
            "Requirement already satisfied: numpy>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorboard) (25.0)\n",
            "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (5.29.5)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (75.2.0)\n",
            "Requirement already satisfied: six>1.9 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (1.17.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard) (3.0.2)\n"
          ]
        }
      ],
      "source": [
        "%pip install tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "c3e1f2b0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c3e1f2b0",
        "outputId": "012a3367-1186-4de8-ad41-e51def526d12"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: ipywidgets in /usr/local/lib/python3.11/dist-packages (7.7.1)\n",
            "Requirement already satisfied: ipykernel>=4.5.1 in /usr/local/lib/python3.11/dist-packages (from ipywidgets) (6.17.1)\n",
            "Requirement already satisfied: ipython-genutils~=0.2.0 in /usr/local/lib/python3.11/dist-packages (from ipywidgets) (0.2.0)\n",
            "Requirement already satisfied: traitlets>=4.3.1 in /usr/local/lib/python3.11/dist-packages (from ipywidgets) (5.7.1)\n",
            "Requirement already satisfied: widgetsnbextension~=3.6.0 in /usr/local/lib/python3.11/dist-packages (from ipywidgets) (3.6.10)\n",
            "Requirement already satisfied: ipython>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from ipywidgets) (7.34.0)\n",
            "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from ipywidgets) (3.0.15)\n",
            "Requirement already satisfied: debugpy>=1.0 in /usr/local/lib/python3.11/dist-packages (from ipykernel>=4.5.1->ipywidgets) (1.8.15)\n",
            "Requirement already satisfied: jupyter-client>=6.1.12 in /usr/local/lib/python3.11/dist-packages (from ipykernel>=4.5.1->ipywidgets) (6.1.12)\n",
            "Requirement already satisfied: matplotlib-inline>=0.1 in /usr/local/lib/python3.11/dist-packages (from ipykernel>=4.5.1->ipywidgets) (0.1.7)\n",
            "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.11/dist-packages (from ipykernel>=4.5.1->ipywidgets) (1.6.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from ipykernel>=4.5.1->ipywidgets) (25.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from ipykernel>=4.5.1->ipywidgets) (5.9.5)\n",
            "Requirement already satisfied: pyzmq>=17 in /usr/local/lib/python3.11/dist-packages (from ipykernel>=4.5.1->ipywidgets) (24.0.1)\n",
            "Requirement already satisfied: tornado>=6.1 in /usr/local/lib/python3.11/dist-packages (from ipykernel>=4.5.1->ipywidgets) (6.4.2)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.11/dist-packages (from ipython>=4.0.0->ipywidgets) (75.2.0)\n",
            "Collecting jedi>=0.16 (from ipython>=4.0.0->ipywidgets)\n",
            "  Downloading jedi-0.19.2-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.11/dist-packages (from ipython>=4.0.0->ipywidgets) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.11/dist-packages (from ipython>=4.0.0->ipywidgets) (0.7.5)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from ipython>=4.0.0->ipywidgets) (3.0.51)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.11/dist-packages (from ipython>=4.0.0->ipywidgets) (2.19.2)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.11/dist-packages (from ipython>=4.0.0->ipywidgets) (0.2.0)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.11/dist-packages (from ipython>=4.0.0->ipywidgets) (4.9.0)\n",
            "Requirement already satisfied: notebook>=4.4.1 in /usr/local/lib/python3.11/dist-packages (from widgetsnbextension~=3.6.0->ipywidgets) (6.5.7)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.11/dist-packages (from jedi>=0.16->ipython>=4.0.0->ipywidgets) (0.8.4)\n",
            "Requirement already satisfied: jupyter-core>=4.6.0 in /usr/local/lib/python3.11/dist-packages (from jupyter-client>=6.1.12->ipykernel>=4.5.1->ipywidgets) (5.8.1)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.11/dist-packages (from jupyter-client>=6.1.12->ipykernel>=4.5.1->ipywidgets) (2.9.0.post0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (3.1.6)\n",
            "Requirement already satisfied: argon2-cffi in /usr/local/lib/python3.11/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (25.1.0)\n",
            "Requirement already satisfied: nbformat in /usr/local/lib/python3.11/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (5.10.4)\n",
            "Requirement already satisfied: nbconvert>=5 in /usr/local/lib/python3.11/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (7.16.6)\n",
            "Requirement already satisfied: Send2Trash>=1.8.0 in /usr/local/lib/python3.11/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.8.3)\n",
            "Requirement already satisfied: terminado>=0.8.3 in /usr/local/lib/python3.11/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.18.1)\n",
            "Requirement already satisfied: prometheus-client in /usr/local/lib/python3.11/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.22.1)\n",
            "Requirement already satisfied: nbclassic>=0.4.7 in /usr/local/lib/python3.11/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.3.1)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.11/dist-packages (from pexpect>4.3->ipython>=4.0.0->ipywidgets) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=4.0.0->ipywidgets) (0.2.13)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.11/dist-packages (from jupyter-core>=4.6.0->jupyter-client>=6.1.12->ipykernel>=4.5.1->ipywidgets) (4.3.8)\n",
            "Requirement already satisfied: notebook-shim>=0.2.3 in /usr/local/lib/python3.11/dist-packages (from nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.2.4)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (4.13.4)\n",
            "Requirement already satisfied: bleach!=5.0.0 in /usr/local/lib/python3.11/dist-packages (from bleach[css]!=5.0.0->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (6.2.0)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.11/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.7.1)\n",
            "Requirement already satisfied: jupyterlab-pygments in /usr/local/lib/python3.11/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.3.0)\n",
            "Requirement already satisfied: markupsafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (3.0.2)\n",
            "Requirement already satisfied: mistune<4,>=2.0.3 in /usr/local/lib/python3.11/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (3.1.3)\n",
            "Requirement already satisfied: nbclient>=0.5.0 in /usr/local/lib/python3.11/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.10.2)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.11/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.5.1)\n",
            "Requirement already satisfied: fastjsonschema>=2.15 in /usr/local/lib/python3.11/dist-packages (from nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (2.21.1)\n",
            "Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.11/dist-packages (from nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (4.24.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.1->jupyter-client>=6.1.12->ipykernel>=4.5.1->ipywidgets) (1.17.0)\n",
            "Requirement already satisfied: argon2-cffi-bindings in /usr/local/lib/python3.11/dist-packages (from argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (21.2.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.11/dist-packages (from bleach!=5.0.0->bleach[css]!=5.0.0->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.5.1)\n",
            "Requirement already satisfied: tinycss2<1.5,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from bleach[css]!=5.0.0->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.4.0)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (2025.4.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.26.0)\n",
            "Requirement already satisfied: jupyter-server<3,>=1.8 in /usr/local/lib/python3.11/dist-packages (from notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.16.0)\n",
            "Requirement already satisfied: cffi>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from argon2-cffi-bindings->argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.17.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (2.7)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (4.14.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (2.22)\n",
            "Requirement already satisfied: anyio>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (4.9.0)\n",
            "Requirement already satisfied: websocket-client in /usr/local/lib/python3.11/dist-packages (from jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.8.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio>=3.1.0->jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio>=3.1.0->jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.3.1)\n",
            "Downloading jedi-0.19.2-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m30.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: jedi\n",
            "Successfully installed jedi-0.19.2\n",
            "Collecting ipympl\n",
            "  Downloading ipympl-0.9.7-py3-none-any.whl.metadata (8.7 kB)\n",
            "Requirement already satisfied: ipython<10 in /usr/local/lib/python3.11/dist-packages (from ipympl) (7.34.0)\n",
            "Requirement already satisfied: ipywidgets<9,>=7.6.0 in /usr/local/lib/python3.11/dist-packages (from ipympl) (7.7.1)\n",
            "Requirement already satisfied: matplotlib<4,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from ipympl) (3.10.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from ipympl) (2.0.2)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (from ipympl) (11.2.1)\n",
            "Requirement already satisfied: traitlets<6 in /usr/local/lib/python3.11/dist-packages (from ipympl) (5.7.1)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.11/dist-packages (from ipython<10->ipympl) (75.2.0)\n",
            "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.11/dist-packages (from ipython<10->ipympl) (0.19.2)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.11/dist-packages (from ipython<10->ipympl) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.11/dist-packages (from ipython<10->ipympl) (0.7.5)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from ipython<10->ipympl) (3.0.51)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.11/dist-packages (from ipython<10->ipympl) (2.19.2)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.11/dist-packages (from ipython<10->ipympl) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.11/dist-packages (from ipython<10->ipympl) (0.1.7)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.11/dist-packages (from ipython<10->ipympl) (4.9.0)\n",
            "Requirement already satisfied: ipykernel>=4.5.1 in /usr/local/lib/python3.11/dist-packages (from ipywidgets<9,>=7.6.0->ipympl) (6.17.1)\n",
            "Requirement already satisfied: ipython-genutils~=0.2.0 in /usr/local/lib/python3.11/dist-packages (from ipywidgets<9,>=7.6.0->ipympl) (0.2.0)\n",
            "Requirement already satisfied: widgetsnbextension~=3.6.0 in /usr/local/lib/python3.11/dist-packages (from ipywidgets<9,>=7.6.0->ipympl) (3.6.10)\n",
            "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from ipywidgets<9,>=7.6.0->ipympl) (3.0.15)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4,>=3.5.0->ipympl) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4,>=3.5.0->ipympl) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4,>=3.5.0->ipympl) (4.59.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4,>=3.5.0->ipympl) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4,>=3.5.0->ipympl) (25.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4,>=3.5.0->ipympl) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4,>=3.5.0->ipympl) (2.9.0.post0)\n",
            "Requirement already satisfied: debugpy>=1.0 in /usr/local/lib/python3.11/dist-packages (from ipykernel>=4.5.1->ipywidgets<9,>=7.6.0->ipympl) (1.8.15)\n",
            "Requirement already satisfied: jupyter-client>=6.1.12 in /usr/local/lib/python3.11/dist-packages (from ipykernel>=4.5.1->ipywidgets<9,>=7.6.0->ipympl) (6.1.12)\n",
            "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.11/dist-packages (from ipykernel>=4.5.1->ipywidgets<9,>=7.6.0->ipympl) (1.6.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from ipykernel>=4.5.1->ipywidgets<9,>=7.6.0->ipympl) (5.9.5)\n",
            "Requirement already satisfied: pyzmq>=17 in /usr/local/lib/python3.11/dist-packages (from ipykernel>=4.5.1->ipywidgets<9,>=7.6.0->ipympl) (24.0.1)\n",
            "Requirement already satisfied: tornado>=6.1 in /usr/local/lib/python3.11/dist-packages (from ipykernel>=4.5.1->ipywidgets<9,>=7.6.0->ipympl) (6.4.2)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.11/dist-packages (from jedi>=0.16->ipython<10->ipympl) (0.8.4)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.11/dist-packages (from pexpect>4.3->ipython<10->ipympl) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython<10->ipympl) (0.2.13)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib<4,>=3.5.0->ipympl) (1.17.0)\n",
            "Requirement already satisfied: notebook>=4.4.1 in /usr/local/lib/python3.11/dist-packages (from widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipympl) (6.5.7)\n",
            "Requirement already satisfied: jupyter-core>=4.6.0 in /usr/local/lib/python3.11/dist-packages (from jupyter-client>=6.1.12->ipykernel>=4.5.1->ipywidgets<9,>=7.6.0->ipympl) (5.8.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipympl) (3.1.6)\n",
            "Requirement already satisfied: argon2-cffi in /usr/local/lib/python3.11/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipympl) (25.1.0)\n",
            "Requirement already satisfied: nbformat in /usr/local/lib/python3.11/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipympl) (5.10.4)\n",
            "Requirement already satisfied: nbconvert>=5 in /usr/local/lib/python3.11/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipympl) (7.16.6)\n",
            "Requirement already satisfied: Send2Trash>=1.8.0 in /usr/local/lib/python3.11/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipympl) (1.8.3)\n",
            "Requirement already satisfied: terminado>=0.8.3 in /usr/local/lib/python3.11/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipympl) (0.18.1)\n",
            "Requirement already satisfied: prometheus-client in /usr/local/lib/python3.11/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipympl) (0.22.1)\n",
            "Requirement already satisfied: nbclassic>=0.4.7 in /usr/local/lib/python3.11/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipympl) (1.3.1)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.11/dist-packages (from jupyter-core>=4.6.0->jupyter-client>=6.1.12->ipykernel>=4.5.1->ipywidgets<9,>=7.6.0->ipympl) (4.3.8)\n",
            "Requirement already satisfied: notebook-shim>=0.2.3 in /usr/local/lib/python3.11/dist-packages (from nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipympl) (0.2.4)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipympl) (4.13.4)\n",
            "Requirement already satisfied: bleach!=5.0.0 in /usr/local/lib/python3.11/dist-packages (from bleach[css]!=5.0.0->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipympl) (6.2.0)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.11/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipympl) (0.7.1)\n",
            "Requirement already satisfied: jupyterlab-pygments in /usr/local/lib/python3.11/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipympl) (0.3.0)\n",
            "Requirement already satisfied: markupsafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipympl) (3.0.2)\n",
            "Requirement already satisfied: mistune<4,>=2.0.3 in /usr/local/lib/python3.11/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipympl) (3.1.3)\n",
            "Requirement already satisfied: nbclient>=0.5.0 in /usr/local/lib/python3.11/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipympl) (0.10.2)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.11/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipympl) (1.5.1)\n",
            "Requirement already satisfied: fastjsonschema>=2.15 in /usr/local/lib/python3.11/dist-packages (from nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipympl) (2.21.1)\n",
            "Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.11/dist-packages (from nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipympl) (4.24.1)\n",
            "Requirement already satisfied: argon2-cffi-bindings in /usr/local/lib/python3.11/dist-packages (from argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipympl) (21.2.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.11/dist-packages (from bleach!=5.0.0->bleach[css]!=5.0.0->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipympl) (0.5.1)\n",
            "Requirement already satisfied: tinycss2<1.5,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from bleach[css]!=5.0.0->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipympl) (1.4.0)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipympl) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipympl) (2025.4.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipympl) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipympl) (0.26.0)\n",
            "Requirement already satisfied: jupyter-server<3,>=1.8 in /usr/local/lib/python3.11/dist-packages (from notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipympl) (1.16.0)\n",
            "Requirement already satisfied: cffi>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from argon2-cffi-bindings->argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipympl) (1.17.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipympl) (2.7)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipympl) (4.14.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipympl) (2.22)\n",
            "Requirement already satisfied: anyio>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipympl) (4.9.0)\n",
            "Requirement already satisfied: websocket-client in /usr/local/lib/python3.11/dist-packages (from jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipympl) (1.8.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio>=3.1.0->jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipympl) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio>=3.1.0->jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipympl) (1.3.1)\n",
            "Downloading ipympl-0.9.7-py3-none-any.whl (515 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m515.7/515.7 kB\u001b[0m \u001b[31m27.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: ipympl\n",
            "Successfully installed ipympl-0.9.7\n"
          ]
        }
      ],
      "source": [
        "%pip install ipywidgets\n",
        "%pip install ipympl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "d9da2183",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "d9da2183",
        "outputId": "00efb28f-f5bf-48c7-8b7f-a06574ba7a7d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/ruflab/shimmer-ssd.git\n",
            "  Cloning https://github.com/ruflab/shimmer-ssd.git to /tmp/pip-req-build-m4f641_e\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/ruflab/shimmer-ssd.git /tmp/pip-req-build-m4f641_e\n",
            "  Resolved https://github.com/ruflab/shimmer-ssd.git to commit 18e0e5de809eb640638cff1b19f29861c103fc3c\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting attributes-to-language@ git+https://github.com/ruflab/attributes-to-language.git@v0.4.0 (from shimmer-ssd==0.4.0.dev0)\n",
            "  Cloning https://github.com/ruflab/attributes-to-language.git (to revision v0.4.0) to /tmp/pip-install-2ph0ii_8/attributes-to-language_fcc29e3453db4035a0accf618e48d964\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/ruflab/attributes-to-language.git /tmp/pip-install-2ph0ii_8/attributes-to-language_fcc29e3453db4035a0accf618e48d964\n",
            "  Running command git checkout -q c3d3c6823a1a8d968a71f786a2384a8b14c3be77\n",
            "  Resolved https://github.com/ruflab/attributes-to-language.git to commit c3d3c6823a1a8d968a71f786a2384a8b14c3be77\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting auto-sbatch@ git+https://github.com/ruflab/auto-sbatch.git@main (from shimmer-ssd==0.4.0.dev0)\n",
            "  Cloning https://github.com/ruflab/auto-sbatch.git (to revision main) to /tmp/pip-install-2ph0ii_8/auto-sbatch_acf4894072024d7184625ec308539348\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/ruflab/auto-sbatch.git /tmp/pip-install-2ph0ii_8/auto-sbatch_acf4894072024d7184625ec308539348\n",
            "  Resolved https://github.com/ruflab/auto-sbatch.git to commit a1db0c14b65149141d0fb2ce32721a3de40ace91\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting cfg-tools@ git+https://github.com/ruflab/cfg-tools.git@main (from shimmer-ssd==0.4.0.dev0)\n",
            "  Cloning https://github.com/ruflab/cfg-tools.git (to revision main) to /tmp/pip-install-2ph0ii_8/cfg-tools_c4d7935e96014cb7bc667fc434f6481a\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/ruflab/cfg-tools.git /tmp/pip-install-2ph0ii_8/cfg-tools_c4d7935e96014cb7bc667fc434f6481a\n",
            "  Resolved https://github.com/ruflab/cfg-tools.git to commit 4cdb0d0c2cd846381b7f2425299b25b3524ff151\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting migrate-ckpt@ git+https://github.com/ruflab/migrate-ckpt.git@v0.2.0 (from shimmer-ssd==0.4.0.dev0)\n",
            "  Cloning https://github.com/ruflab/migrate-ckpt.git (to revision v0.2.0) to /tmp/pip-install-2ph0ii_8/migrate-ckpt_bdb9352b57ab41a3803e8222a5e709cd\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/ruflab/migrate-ckpt.git /tmp/pip-install-2ph0ii_8/migrate-ckpt_bdb9352b57ab41a3803e8222a5e709cd\n",
            "  Resolved https://github.com/ruflab/migrate-ckpt.git to commit 6a78a0cfd6abafa77310307025581eb2d62bcde7\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting shimmer@ git+https://github.com/ruflab/shimmer.git@main (from shimmer-ssd==0.4.0.dev0)\n",
            "  Cloning https://github.com/ruflab/shimmer.git (to revision main) to /tmp/pip-install-2ph0ii_8/shimmer_8753f378a12d4334a042f3b6d9f968e0\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/ruflab/shimmer.git /tmp/pip-install-2ph0ii_8/shimmer_8753f378a12d4334a042f3b6d9f968e0\n",
            "  Resolved https://github.com/ruflab/shimmer.git to commit 7ee9239bb5b6bcf51658d5396d8f44b724e6b5b9\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting simple-shapes-dataset@ git+https://github.com/ruflab/simple-shapes-dataset.git@main (from shimmer-ssd==0.4.0.dev0)\n",
            "  Cloning https://github.com/ruflab/simple-shapes-dataset.git (to revision main) to /tmp/pip-install-2ph0ii_8/simple-shapes-dataset_79c0143996cb4ae4b82bd93c563bf1f1\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/ruflab/simple-shapes-dataset.git /tmp/pip-install-2ph0ii_8/simple-shapes-dataset_79c0143996cb4ae4b82bd93c563bf1f1\n",
            "  Resolved https://github.com/ruflab/simple-shapes-dataset.git to commit b9b1bedcdd6d88aedf03a0575c7ab1719bf1d993\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: click<9.0.0,>=8.1.3 in /usr/local/lib/python3.11/dist-packages (from shimmer-ssd==0.4.0.dev0) (8.2.1)\n",
            "Collecting lightning<3.0.0,>=2.1.0 (from shimmer-ssd==0.4.0.dev0)\n",
            "  Downloading lightning-2.5.2-py3-none-any.whl.metadata (38 kB)\n",
            "Requirement already satisfied: matplotlib<4.0.0,>=3.7.0 in /usr/local/lib/python3.11/dist-packages (from shimmer-ssd==0.4.0.dev0) (3.10.0)\n",
            "Collecting numpy<2.0,>=1.26 (from shimmer-ssd==0.4.0.dev0)\n",
            "  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: opencv-python<5.0.0.0,>=4.7.0.72 in /usr/local/lib/python3.11/dist-packages (from shimmer-ssd==0.4.0.dev0) (4.11.0.86)\n",
            "Collecting pillow<11.0.0,>=10.3.0 (from shimmer-ssd==0.4.0.dev0)\n",
            "  Downloading pillow-10.4.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.6.0 in /usr/local/lib/python3.11/dist-packages (from shimmer-ssd==0.4.0.dev0) (2.11.7)\n",
            "Requirement already satisfied: rich<14.0.0,>=13.4.2 in /usr/local/lib/python3.11/dist-packages (from shimmer-ssd==0.4.0.dev0) (13.9.4)\n",
            "Collecting ruamel-yaml<1.0.0,>=0.18.6 (from shimmer-ssd==0.4.0.dev0)\n",
            "  Downloading ruamel.yaml-0.18.14-py3-none-any.whl.metadata (24 kB)\n",
            "Requirement already satisfied: tokenizers<1.0.0,>=0.19.1 in /usr/local/lib/python3.11/dist-packages (from shimmer-ssd==0.4.0.dev0) (0.21.2)\n",
            "Requirement already satisfied: torch<3.0.0,>=2.0.1 in /usr/local/lib/python3.11/dist-packages (from shimmer-ssd==0.4.0.dev0) (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision<1.0.0,>=0.15.2 in /usr/local/lib/python3.11/dist-packages (from shimmer-ssd==0.4.0.dev0) (0.21.0+cu124)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.44.2 in /usr/local/lib/python3.11/dist-packages (from shimmer-ssd==0.4.0.dev0) (4.67.1)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.44.2 in /usr/local/lib/python3.11/dist-packages (from shimmer-ssd==0.4.0.dev0) (4.53.2)\n",
            "Requirement already satisfied: wandb<1.0.0,>=0.18.3 in /usr/local/lib/python3.11/dist-packages (from shimmer-ssd==0.4.0.dev0) (0.21.0)\n",
            "Requirement already satisfied: PyYAML<8.0,>=5.4 in /usr/local/lib/python3.11/dist-packages (from lightning<3.0.0,>=2.1.0->shimmer-ssd==0.4.0.dev0) (6.0.2)\n",
            "Requirement already satisfied: fsspec<2027.0,>=2022.5.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<2027.0,>=2022.5.0->lightning<3.0.0,>=2.1.0->shimmer-ssd==0.4.0.dev0) (2025.3.2)\n",
            "Collecting lightning-utilities<2.0,>=0.10.0 (from lightning<3.0.0,>=2.1.0->shimmer-ssd==0.4.0.dev0)\n",
            "  Downloading lightning_utilities-0.14.3-py3-none-any.whl.metadata (5.6 kB)\n",
            "Requirement already satisfied: packaging<27.0,>=20.0 in /usr/local/lib/python3.11/dist-packages (from lightning<3.0.0,>=2.1.0->shimmer-ssd==0.4.0.dev0) (25.0)\n",
            "Collecting torchmetrics<3.0,>=0.7.0 (from lightning<3.0.0,>=2.1.0->shimmer-ssd==0.4.0.dev0)\n",
            "  Downloading torchmetrics-1.7.4-py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: typing-extensions<6.0,>=4.4.0 in /usr/local/lib/python3.11/dist-packages (from lightning<3.0.0,>=2.1.0->shimmer-ssd==0.4.0.dev0) (4.14.1)\n",
            "Collecting pytorch-lightning (from lightning<3.0.0,>=2.1.0->shimmer-ssd==0.4.0.dev0)\n",
            "  Downloading pytorch_lightning-2.5.2-py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4.0.0,>=3.7.0->shimmer-ssd==0.4.0.dev0) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4.0.0,>=3.7.0->shimmer-ssd==0.4.0.dev0) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4.0.0,>=3.7.0->shimmer-ssd==0.4.0.dev0) (4.59.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4.0.0,>=3.7.0->shimmer-ssd==0.4.0.dev0) (1.4.8)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4.0.0,>=3.7.0->shimmer-ssd==0.4.0.dev0) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4.0.0,>=3.7.0->shimmer-ssd==0.4.0.dev0) (2.9.0.post0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.6.0->shimmer-ssd==0.4.0.dev0) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.6.0->shimmer-ssd==0.4.0.dev0) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.6.0->shimmer-ssd==0.4.0.dev0) (0.4.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich<14.0.0,>=13.4.2->shimmer-ssd==0.4.0.dev0) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich<14.0.0,>=13.4.2->shimmer-ssd==0.4.0.dev0) (2.19.2)\n",
            "Collecting ruamel.yaml.clib>=0.2.7 (from ruamel-yaml<1.0.0,>=0.18.6->shimmer-ssd==0.4.0.dev0)\n",
            "  Downloading ruamel.yaml.clib-0.2.12-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.11/dist-packages (from tokenizers<1.0.0,>=0.19.1->shimmer-ssd==0.4.0.dev0) (0.33.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch<3.0.0,>=2.0.1->shimmer-ssd==0.4.0.dev0) (3.18.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch<3.0.0,>=2.0.1->shimmer-ssd==0.4.0.dev0) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch<3.0.0,>=2.0.1->shimmer-ssd==0.4.0.dev0) (3.1.6)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch<3.0.0,>=2.0.1->shimmer-ssd==0.4.0.dev0)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch<3.0.0,>=2.0.1->shimmer-ssd==0.4.0.dev0)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch<3.0.0,>=2.0.1->shimmer-ssd==0.4.0.dev0)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch<3.0.0,>=2.0.1->shimmer-ssd==0.4.0.dev0)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch<3.0.0,>=2.0.1->shimmer-ssd==0.4.0.dev0)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch<3.0.0,>=2.0.1->shimmer-ssd==0.4.0.dev0)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch<3.0.0,>=2.0.1->shimmer-ssd==0.4.0.dev0)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch<3.0.0,>=2.0.1->shimmer-ssd==0.4.0.dev0)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch<3.0.0,>=2.0.1->shimmer-ssd==0.4.0.dev0)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch<3.0.0,>=2.0.1->shimmer-ssd==0.4.0.dev0) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch<3.0.0,>=2.0.1->shimmer-ssd==0.4.0.dev0) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3.0.0,>=2.0.1->shimmer-ssd==0.4.0.dev0) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch<3.0.0,>=2.0.1->shimmer-ssd==0.4.0.dev0)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch<3.0.0,>=2.0.1->shimmer-ssd==0.4.0.dev0) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch<3.0.0,>=2.0.1->shimmer-ssd==0.4.0.dev0) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch<3.0.0,>=2.0.1->shimmer-ssd==0.4.0.dev0) (1.3.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.44.2->shimmer-ssd==0.4.0.dev0) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.44.2->shimmer-ssd==0.4.0.dev0) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.44.2->shimmer-ssd==0.4.0.dev0) (0.5.3)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb<1.0.0,>=0.18.3->shimmer-ssd==0.4.0.dev0) (3.1.44)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.11/dist-packages (from wandb<1.0.0,>=0.18.3->shimmer-ssd==0.4.0.dev0) (4.3.8)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 in /usr/local/lib/python3.11/dist-packages (from wandb<1.0.0,>=0.18.3->shimmer-ssd==0.4.0.dev0) (5.29.5)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb<1.0.0,>=0.18.3->shimmer-ssd==0.4.0.dev0) (2.33.0)\n",
            "Requirement already satisfied: pandas<3.0.0,>=2.2.2 in /usr/local/lib/python3.11/dist-packages (from shimmer@ git+https://github.com/ruflab/shimmer.git@main->shimmer-ssd==0.4.0.dev0) (2.2.2)\n",
            "INFO: pip is looking at multiple versions of simple-shapes-dataset to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting torchvision<1.0.0,>=0.15.2 (from shimmer-ssd==0.4.0.dev0)\n",
            "  Downloading torchvision-0.22.1-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (6.1 kB)\n",
            "Collecting torch<3.0.0,>=2.0.1 (from shimmer-ssd==0.4.0.dev0)\n",
            "  Downloading torch-2.7.1-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (29 kB)\n",
            "Collecting sympy>=1.13.3 (from torch<3.0.0,>=2.0.1->shimmer-ssd==0.4.0.dev0)\n",
            "  Downloading sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.6.77 (from torch<3.0.0,>=2.0.1->shimmer-ssd==0.4.0.dev0)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.6.77-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.6.77 (from torch<3.0.0,>=2.0.1->shimmer-ssd==0.4.0.dev0)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.6.80 (from torch<3.0.0,>=2.0.1->shimmer-ssd==0.4.0.dev0)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.6.80-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.5.1.17 (from torch<3.0.0,>=2.0.1->shimmer-ssd==0.4.0.dev0)\n",
            "  Downloading nvidia_cudnn_cu12-9.5.1.17-py3-none-manylinux_2_28_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12 (from nvidia-cudnn-cu12==9.1.0.70->torch<3.0.0,>=2.0.1->shimmer-ssd==0.4.0.dev0)\n",
            "  Downloading nvidia_cublas_cu12-12.6.4.1-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.3.0.4 (from torch<3.0.0,>=2.0.1->shimmer-ssd==0.4.0.dev0)\n",
            "  Downloading nvidia_cufft_cu12-11.3.0.4-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.7.77 (from torch<3.0.0,>=2.0.1->shimmer-ssd==0.4.0.dev0)\n",
            "  Downloading nvidia_curand_cu12-10.3.7.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.7.1.2 (from torch<3.0.0,>=2.0.1->shimmer-ssd==0.4.0.dev0)\n",
            "  Downloading nvidia_cusolver_cu12-11.7.1.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12 (from nvidia-cusolver-cu12==11.6.1.9->torch<3.0.0,>=2.0.1->shimmer-ssd==0.4.0.dev0)\n",
            "  Downloading nvidia_cusparse_cu12-12.5.4.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparselt-cu12==0.6.3 (from torch<3.0.0,>=2.0.1->shimmer-ssd==0.4.0.dev0)\n",
            "  Downloading nvidia_cusparselt_cu12-0.6.3-py3-none-manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
            "Collecting nvidia-nccl-cu12==2.26.2 (from torch<3.0.0,>=2.0.1->shimmer-ssd==0.4.0.dev0)\n",
            "  Downloading nvidia_nccl_cu12-2.26.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.0 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.6.77 (from torch<3.0.0,>=2.0.1->shimmer-ssd==0.4.0.dev0)\n",
            "  Downloading nvidia_nvtx_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.6.1.9->torch<3.0.0,>=2.0.1->shimmer-ssd==0.4.0.dev0)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.6.85-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufile-cu12==1.11.1.6 (from torch<3.0.0,>=2.0.1->shimmer-ssd==0.4.0.dev0)\n",
            "  Downloading nvidia_cufile_cu12-1.11.1.6-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting triton==3.3.1 (from torch<3.0.0,>=2.0.1->shimmer-ssd==0.4.0.dev0)\n",
            "  Downloading triton-3.3.1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: setuptools>=40.8.0 in /usr/local/lib/python3.11/dist-packages (from triton==3.3.1->torch<3.0.0,>=2.0.1->shimmer-ssd==0.4.0.dev0) (75.2.0)\n",
            "Collecting torchvision<1.0.0,>=0.15.2 (from shimmer-ssd==0.4.0.dev0)\n",
            "  Downloading torchvision-0.22.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (6.1 kB)\n",
            "Collecting torch<3.0.0,>=2.0.1 (from shimmer-ssd==0.4.0.dev0)\n",
            "  Downloading torch-2.7.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (29 kB)\n",
            "Collecting triton==3.3.0 (from torch<3.0.0,>=2.0.1->shimmer-ssd==0.4.0.dev0)\n",
            "  Downloading triton-3.3.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting torchvision<1.0.0,>=0.15.2 (from shimmer-ssd==0.4.0.dev0)\n",
            "  Downloading torchvision-0.21.0-cp311-cp311-manylinux1_x86_64.whl.metadata (6.1 kB)\n",
            "  Downloading torchvision-0.20.1-cp311-cp311-manylinux1_x86_64.whl.metadata (6.1 kB)\n",
            "Collecting torch<3.0.0,>=2.0.1 (from shimmer-ssd==0.4.0.dev0)\n",
            "  Downloading torch-2.5.1-cp311-cp311-manylinux1_x86_64.whl.metadata (28 kB)\n",
            "Collecting triton==3.1.0 (from torch<3.0.0,>=2.0.1->shimmer-ssd==0.4.0.dev0)\n",
            "  Downloading triton-3.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.3 kB)\n",
            "Collecting tqdm<5.0.0,>=4.44.2 (from shimmer-ssd==0.4.0.dev0)\n",
            "  Downloading tqdm-4.66.6-py3-none-any.whl.metadata (57 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.6/57.6 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<2027.0,>=2022.5.0->lightning<3.0.0,>=2.1.0->shimmer-ssd==0.4.0.dev0) (3.11.15)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb<1.0.0,>=0.18.3->shimmer-ssd==0.4.0.dev0) (4.0.12)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<1.0.0,>=0.19.1->shimmer-ssd==0.4.0.dev0) (1.1.5)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich<14.0.0,>=13.4.2->shimmer-ssd==0.4.0.dev0) (0.1.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0.0,>=2.2.2->shimmer@ git+https://github.com/ruflab/shimmer.git@main->shimmer-ssd==0.4.0.dev0) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0.0,>=2.2.2->shimmer@ git+https://github.com/ruflab/shimmer.git@main->shimmer-ssd==0.4.0.dev0) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib<4.0.0,>=3.7.0->shimmer-ssd==0.4.0.dev0) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers<5.0.0,>=4.44.2->shimmer-ssd==0.4.0.dev0) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers<5.0.0,>=4.44.2->shimmer-ssd==0.4.0.dev0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers<5.0.0,>=4.44.2->shimmer-ssd==0.4.0.dev0) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers<5.0.0,>=4.44.2->shimmer-ssd==0.4.0.dev0) (2025.7.14)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch<3.0.0,>=2.0.1->shimmer-ssd==0.4.0.dev0) (3.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2027.0,>=2022.5.0->lightning<3.0.0,>=2.1.0->shimmer-ssd==0.4.0.dev0) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2027.0,>=2022.5.0->lightning<3.0.0,>=2.1.0->shimmer-ssd==0.4.0.dev0) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2027.0,>=2022.5.0->lightning<3.0.0,>=2.1.0->shimmer-ssd==0.4.0.dev0) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2027.0,>=2022.5.0->lightning<3.0.0,>=2.1.0->shimmer-ssd==0.4.0.dev0) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2027.0,>=2022.5.0->lightning<3.0.0,>=2.1.0->shimmer-ssd==0.4.0.dev0) (6.6.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2027.0,>=2022.5.0->lightning<3.0.0,>=2.1.0->shimmer-ssd==0.4.0.dev0) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2027.0,>=2022.5.0->lightning<3.0.0,>=2.1.0->shimmer-ssd==0.4.0.dev0) (1.20.1)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb<1.0.0,>=0.18.3->shimmer-ssd==0.4.0.dev0) (5.0.2)\n",
            "Downloading lightning-2.5.2-py3-none-any.whl (821 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m821.1/821.1 kB\u001b[0m \u001b[31m38.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m71.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pillow-10.4.0-cp311-cp311-manylinux_2_28_x86_64.whl (4.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m85.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ruamel.yaml-0.18.14-py3-none-any.whl (118 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.6/118.6 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m67.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m36.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m50.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m20.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m105.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchvision-0.20.1-cp311-cp311-manylinux1_x86_64.whl (7.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m131.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torch-2.5.1-cp311-cp311-manylinux1_x86_64.whl (906.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m906.5/906.5 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading triton-3.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (209.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.5/209.5 MB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tqdm-4.66.6-py3-none-any.whl (78 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.3/78.3 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lightning_utilities-0.14.3-py3-none-any.whl (28 kB)\n",
            "Downloading ruamel.yaml.clib-0.2.12-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (739 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m739.1/739.1 kB\u001b[0m \u001b[31m53.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchmetrics-1.7.4-py3-none-any.whl (963 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m963.5/963.5 kB\u001b[0m \u001b[31m64.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pytorch_lightning-2.5.2-py3-none-any.whl (825 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m825.4/825.4 kB\u001b[0m \u001b[31m56.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: shimmer-ssd, shimmer, simple-shapes-dataset, attributes-to-language, auto-sbatch, cfg-tools, migrate-ckpt\n",
            "  Building wheel for shimmer-ssd (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for shimmer-ssd: filename=shimmer_ssd-0.4.0.dev0-py3-none-any.whl size=49781 sha256=ba07fa45b5d31069b95fa2fa84c5d1970b57dcbcd4b38dd75755703b80139a92\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-dg1ym3ul/wheels/19/47/8e/36d34f5575994a446e6fd4545ab3a1bf37a9feb68b4f6c6a38\n",
            "  Building wheel for shimmer (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for shimmer: filename=shimmer-0.6.0.dev0-py3-none-any.whl size=37219 sha256=d83bf41b0f70f49b48f694b475af87d0887b2b1d51503eac58d944e55fc99f04\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-dg1ym3ul/wheels/33/bb/46/bd4230c98a7b3c1c53eec2c714103e79b98081beebccb8fb44\n",
            "  Building wheel for simple-shapes-dataset (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for simple-shapes-dataset: filename=simple_shapes_dataset-1.0.0-py3-none-any.whl size=38727 sha256=d906846b2c60d712239198ae17613ee3711a5342148b52117620ed37fab58dba\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-dg1ym3ul/wheels/80/31/20/49a8e5c50ed1e32a508fa94216228811a495c6dfadd9a7e8fa\n",
            "  Building wheel for attributes-to-language (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for attributes-to-language: filename=attributes_to_language-0.4.0-py3-none-any.whl size=17854 sha256=1acd5ab3f2fbe11d8a77bc284b8ad8c5a0e8361559fcc02f2f7f47be95506f96\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-dg1ym3ul/wheels/f9/0b/17/bad0dcc2f97d1eef47589aaf91038478b524ce15d96202696e\n",
            "  Building wheel for auto-sbatch (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for auto-sbatch: filename=auto_sbatch-0.4.1-py3-none-any.whl size=9921 sha256=a049172af17022047139354373cf832fa686b006704c1c062a682c479a31ee6e\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-dg1ym3ul/wheels/bd/2f/a7/fdbe1042932fc383917dcd95938bbdc3cf4711f3f608676348\n",
            "  Building wheel for cfg-tools (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for cfg-tools: filename=cfg_tools-0.3.1-py3-none-any.whl size=7919 sha256=4ca88ce24625d436eded90ab34e3dd703a7b45ae6cc7bd897fb02b001651bdc1\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-dg1ym3ul/wheels/53/e3/7e/e5bffe3e1c65a214b0582647c95e66202382ed9d4c155870ce\n",
            "  Building wheel for migrate-ckpt (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for migrate-ckpt: filename=migrate_ckpt-0.1.1-py3-none-any.whl size=3911 sha256=515786fca914bc39d93bff60d7374e688e36715c2cdac09c214ee741bedf806a\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-dg1ym3ul/wheels/bc/fd/3d/b2ea89ee2df4b846e8247e8c8905dc8d1204596e7495840db2\n",
            "Successfully built shimmer-ssd shimmer simple-shapes-dataset attributes-to-language auto-sbatch cfg-tools migrate-ckpt\n",
            "Installing collected packages: triton, tqdm, ruamel.yaml.clib, pillow, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, numpy, migrate-ckpt, lightning-utilities, auto-sbatch, ruamel-yaml, nvidia-cusparse-cu12, nvidia-cudnn-cu12, attributes-to-language, nvidia-cusolver-cu12, cfg-tools, torch, torchvision, torchmetrics, pytorch-lightning, lightning, simple-shapes-dataset, shimmer, shimmer-ssd\n",
            "  Attempting uninstall: triton\n",
            "    Found existing installation: triton 3.2.0\n",
            "    Uninstalling triton-3.2.0:\n",
            "      Successfully uninstalled triton-3.2.0\n",
            "  Attempting uninstall: tqdm\n",
            "    Found existing installation: tqdm 4.67.1\n",
            "    Uninstalling tqdm-4.67.1:\n",
            "      Successfully uninstalled tqdm-4.67.1\n",
            "  Attempting uninstall: pillow\n",
            "    Found existing installation: pillow 11.2.1\n",
            "    Uninstalling pillow-11.2.1:\n",
            "      Successfully uninstalled pillow-11.2.1\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.6.0+cu124\n",
            "    Uninstalling torch-2.6.0+cu124:\n",
            "      Successfully uninstalled torch-2.6.0+cu124\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.21.0+cu124\n",
            "    Uninstalling torchvision-0.21.0+cu124:\n",
            "      Successfully uninstalled torchvision-0.21.0+cu124\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "dataproc-spark-connect 0.8.2 requires tqdm>=4.67, but you have tqdm 4.66.6 which is incompatible.\n",
            "torchaudio 2.6.0+cu124 requires torch==2.6.0, but you have torch 2.5.1 which is incompatible.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed attributes-to-language-0.4.0 auto-sbatch-0.4.1 cfg-tools-0.3.1 lightning-2.5.2 lightning-utilities-0.14.3 migrate-ckpt-0.1.1 numpy-1.26.4 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 pillow-10.4.0 pytorch-lightning-2.5.2 ruamel-yaml-0.18.14 ruamel.yaml.clib-0.2.12 shimmer-0.6.0.dev0 shimmer-ssd-0.4.0.dev0 simple-shapes-dataset-1.0.0 torch-2.5.1 torchmetrics-1.7.4 torchvision-0.20.1 tqdm-4.66.6 triton-3.1.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "PIL",
                  "numpy",
                  "torch",
                  "torchgen",
                  "tqdm"
                ]
              },
              "id": "9af81d99ecae4d92a5dcda82341b321e"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "!pip install \"git+https://github.com/ruflab/shimmer-ssd.git\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "a206da95",
      "metadata": {
        "id": "a206da95"
      },
      "outputs": [],
      "source": [
        "from google.colab import output\n",
        "output.enable_custom_widget_manager()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "b16202fa",
      "metadata": {
        "id": "b16202fa"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "4a58f24c",
      "metadata": {
        "id": "4a58f24c"
      },
      "outputs": [],
      "source": [
        "from collections.abc import Mapping, Sequence\n",
        "from pathlib import Path\n",
        "from typing import Any, cast\n",
        "\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from lightning.pytorch import Callback, Trainer, seed_everything\n",
        "from lightning.pytorch.callbacks import ModelCheckpoint\n",
        "from lightning.pytorch.loggers import TensorBoardLogger\n",
        "from shimmer import DomainModule, LossOutput\n",
        "from shimmer.modules.domain import DomainModule\n",
        "from shimmer.modules.global_workspace import GlobalWorkspaceFusion, SchedulerArgs\n",
        "from shimmer.modules.vae import (\n",
        "    VAE,\n",
        "    VAEDecoder,\n",
        "    VAEEncoder,\n",
        "    gaussian_nll,\n",
        "    kl_divergence_loss,\n",
        ")\n",
        "from shimmer_ssd import DEBUG_MODE, LOGGER, PROJECT_DIR\n",
        "from shimmer_ssd.config import DomainModuleVariant, LoadedDomainConfig, load_config\n",
        "from shimmer_ssd.dataset.pre_process import TokenizeCaptions\n",
        "from shimmer_ssd.logging import (\n",
        "    LogAttributesCallback,\n",
        "    LogGWImagesCallback,\n",
        "    LogVisualCallback,\n",
        "    batch_to_device,\n",
        ")\n",
        "from shimmer_ssd.modules.domains import load_pretrained_domains\n",
        "from shimmer_ssd.modules.domains.visual import VisualLatentDomainModule\n",
        "from shimmer_ssd.modules.vae import RAEDecoder, RAEEncoder\n",
        "from tokenizers.implementations.byte_level_bpe import ByteLevelBPETokenizer\n",
        "from torch import nn\n",
        "from torch.nn.functional import mse_loss\n",
        "from torch.optim.lr_scheduler import OneCycleLR\n",
        "from torch.optim.optimizer import Optimizer\n",
        "from torchvision.utils import make_grid\n",
        "\n",
        "from simple_shapes_dataset import SimpleShapesDataModule, get_default_domains"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e2fedac8",
      "metadata": {
        "id": "e2fedac8"
      },
      "source": [
        "# Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cdba1f9b",
      "metadata": {
        "id": "cdba1f9b"
      },
      "source": [
        "The dataset we will use is called the Simple Shapes Dataset. It consists of colored shapes described in 3 different modalities.\n",
        "\n",
        "* **Attributes**: A vector of attributes giving perfect description of the shape:\n",
        "\n",
        "| Attributes    | Description                                               |\n",
        "|---------------|-----------------------------------------------------------|\n",
        "| cls           | type of shape: triangle, egg, diamond                     |\n",
        "| x             | position of the shape on the x axis                       |\n",
        "| y             | position of the shape on the y axis                       |\n",
        "| size          | size of the shape                                         |\n",
        "| rotation      | angle of the shape in radians (0 is pointing to the top)  |\n",
        "| color         | RGB color of the shape                                    |\n",
        "\n",
        "\n",
        "* **Vision**: A 32x32 RGB image of the shape on a black background\n",
        "* **Text**: A caption in english describing the shape\n",
        "\n",
        "<img src=\"https://lmaytie.com/tutorial/dataset_examples.png?\" width=\"500\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4110c5e9",
      "metadata": {
        "id": "4110c5e9"
      },
      "source": [
        "Let download the dataset directly with:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "894383d1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "894383d1",
        "outputId": "6191edd2-3115-4946-9f24-840c94f443c1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-07-22 11:32:06.419432: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1753183926.440029    3798 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1753183926.446291    3798 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "Downloading in ..\n",
            "  0% 0.00/358M [00:00<?, ?B/s]\n",
            "  0% 8.19k/358M [00:00<2:21:26, 42.1kB/s]\n",
            "  0% 41.0k/358M [00:00<51:32, 116kB/s]   \n",
            "  0% 73.7k/358M [00:00<42:54, 139kB/s]\n",
            "  0% 156k/358M [00:00<23:59, 248kB/s] \n",
            "  0% 270k/358M [00:00<16:06, 370kB/s]\n",
            "  0% 557k/358M [00:01<08:02, 741kB/s]\n",
            "  0% 1.11M/358M [00:01<04:09, 1.43MB/s]\n",
            "  1% 2.24M/358M [00:01<02:06, 2.81MB/s]\n",
            "  1% 4.51M/358M [00:01<01:23, 4.22MB/s]\n",
            "  2% 8.57M/358M [00:02<00:41, 8.40MB/s]\n",
            "  3% 11.1M/358M [00:02<00:37, 9.36MB/s]\n",
            "  3% 12.1M/358M [00:02<00:41, 8.33MB/s]\n",
            "  4% 15.0M/358M [00:02<00:33, 10.2MB/s]\n",
            "  5% 17.3M/358M [00:02<00:32, 10.6MB/s]\n",
            "  5% 19.5M/358M [00:03<00:31, 10.9MB/s]\n",
            "  6% 21.8M/358M [00:03<00:30, 11.1MB/s]\n",
            "  7% 24.2M/358M [00:03<00:29, 11.3MB/s]\n",
            "  7% 26.5M/358M [00:03<00:28, 11.5MB/s]\n",
            "  8% 28.9M/358M [00:03<00:28, 11.7MB/s]\n",
            "  9% 31.3M/358M [00:04<00:27, 11.9MB/s]\n",
            "  9% 33.7M/358M [00:04<00:26, 12.0MB/s]\n",
            " 10% 36.2M/358M [00:04<00:26, 12.2MB/s]\n",
            " 11% 38.6M/358M [00:04<00:26, 12.3MB/s]\n",
            " 12% 41.1M/358M [00:04<00:25, 12.4MB/s]\n",
            " 12% 43.6M/358M [00:05<00:25, 12.5MB/s]\n",
            " 13% 46.2M/358M [00:05<00:24, 12.6MB/s]\n",
            " 14% 48.7M/358M [00:05<00:24, 12.7MB/s]\n",
            " 14% 51.2M/358M [00:05<00:23, 12.9MB/s]\n",
            " 15% 53.8M/358M [00:05<00:23, 12.9MB/s]\n",
            " 16% 56.4M/358M [00:06<00:23, 13.0MB/s]\n",
            " 16% 59.0M/358M [00:06<00:22, 13.1MB/s]\n",
            " 17% 61.6M/358M [00:06<00:22, 13.2MB/s]\n",
            " 18% 64.2M/358M [00:06<00:22, 13.2MB/s]\n",
            " 19% 66.8M/358M [00:06<00:22, 13.2MB/s]\n",
            " 19% 68.1M/358M [00:07<00:30, 9.57MB/s]\n",
            " 20% 70.1M/358M [00:07<00:29, 9.74MB/s]\n",
            " 20% 71.7M/358M [00:08<01:22, 3.45MB/s]\n",
            " 21% 74.3M/358M [00:08<00:59, 4.73MB/s]\n",
            " 21% 75.7M/358M [00:09<01:22, 3.44MB/s]\n",
            " 22% 78.3M/358M [00:09<00:58, 4.74MB/s]\n",
            " 22% 79.7M/358M [00:10<01:21, 3.41MB/s]\n",
            " 23% 82.4M/358M [00:10<00:58, 4.74MB/s]\n",
            " 23% 83.7M/358M [00:11<01:20, 3.41MB/s]\n",
            " 24% 86.4M/358M [00:11<00:57, 4.74MB/s]\n",
            " 25% 87.7M/358M [00:12<01:19, 3.41MB/s]\n",
            " 25% 90.4M/358M [00:12<00:56, 4.75MB/s]\n",
            " 26% 91.7M/358M [00:13<01:18, 3.41MB/s]\n",
            " 26% 94.4M/358M [00:13<00:55, 4.75MB/s]\n",
            " 27% 95.7M/358M [00:14<01:16, 3.40MB/s]\n",
            " 28% 98.4M/358M [00:14<00:54, 4.75MB/s]\n",
            " 28% 99.7M/358M [00:15<01:15, 3.40MB/s]\n",
            " 29% 102M/358M [00:15<00:53, 4.74MB/s] \n",
            " 29% 104M/358M [00:16<01:14, 3.40MB/s]\n",
            " 30% 106M/358M [00:16<00:52, 4.75MB/s]\n",
            " 30% 108M/358M [00:17<01:13, 3.40MB/s]\n",
            " 31% 110M/358M [00:17<00:51, 4.76MB/s]\n",
            " 31% 112M/358M [00:18<01:12, 3.39MB/s]\n",
            " 32% 114M/358M [00:18<00:51, 4.75MB/s]\n",
            " 32% 116M/358M [00:19<01:11, 3.38MB/s]\n",
            " 33% 118M/358M [00:19<00:50, 4.74MB/s]\n",
            " 33% 120M/358M [00:20<01:10, 3.40MB/s]\n",
            " 34% 122M/358M [00:20<00:49, 4.76MB/s]\n",
            " 35% 124M/358M [00:21<01:08, 3.40MB/s]\n",
            " 35% 126M/358M [00:21<00:48, 4.76MB/s]\n",
            " 36% 128M/358M [00:22<01:07, 3.40MB/s]\n",
            " 36% 130M/358M [00:22<00:47, 4.76MB/s]\n",
            " 37% 132M/358M [00:23<01:06, 3.39MB/s]\n",
            " 38% 134M/358M [00:23<00:46, 4.75MB/s]\n",
            " 38% 136M/358M [00:24<01:05, 3.39MB/s]\n",
            " 39% 138M/358M [00:24<00:46, 4.76MB/s]\n",
            " 39% 140M/358M [00:25<01:04, 3.38MB/s]\n",
            " 39% 141M/358M [00:25<00:56, 3.84MB/s]\n",
            " 40% 144M/358M [00:26<00:59, 3.62MB/s]\n",
            " 41% 146M/358M [00:26<00:48, 4.41MB/s]\n",
            " 41% 148M/358M [00:27<00:39, 5.27MB/s]\n",
            " 41% 148M/358M [00:27<00:58, 3.57MB/s]\n",
            " 42% 150M/358M [00:27<00:50, 4.11MB/s]\n",
            " 42% 152M/358M [00:28<00:39, 5.24MB/s]\n",
            " 43% 152M/358M [00:28<00:59, 3.47MB/s]\n",
            " 43% 154M/358M [00:28<00:50, 4.05MB/s]\n",
            " 44% 156M/358M [00:29<01:02, 3.25MB/s]\n",
            " 44% 158M/358M [00:29<00:45, 4.37MB/s]\n",
            " 45% 160M/358M [00:30<00:57, 3.43MB/s]\n",
            " 45% 162M/358M [00:30<00:43, 4.47MB/s]\n",
            " 46% 163M/358M [00:31<00:39, 4.91MB/s]\n",
            " 46% 164M/358M [00:31<00:58, 3.33MB/s]\n",
            " 46% 165M/358M [00:31<00:50, 3.84MB/s]\n",
            " 47% 167M/358M [00:32<00:41, 4.62MB/s]\n",
            " 47% 168M/358M [00:32<01:02, 3.04MB/s]\n",
            " 47% 169M/358M [00:33<00:48, 3.87MB/s]\n",
            " 48% 171M/358M [00:33<00:39, 4.71MB/s]\n",
            " 48% 172M/358M [00:33<00:56, 3.27MB/s]\n",
            " 48% 173M/358M [00:33<00:44, 4.17MB/s]\n",
            " 49% 175M/358M [00:34<00:36, 5.03MB/s]\n",
            " 49% 176M/358M [00:34<00:55, 3.29MB/s]\n",
            " 50% 177M/358M [00:34<00:42, 4.23MB/s]\n",
            " 50% 179M/358M [00:35<00:34, 5.12MB/s]\n",
            " 50% 180M/358M [00:35<00:55, 3.21MB/s]\n",
            " 51% 181M/358M [00:35<00:42, 4.17MB/s]\n",
            " 51% 183M/358M [00:36<00:34, 5.10MB/s]\n",
            " 51% 184M/358M [00:36<00:54, 3.21MB/s]\n",
            " 52% 185M/358M [00:36<00:41, 4.12MB/s]\n",
            " 52% 187M/358M [00:37<00:33, 5.09MB/s]\n",
            " 53% 188M/358M [00:37<00:52, 3.21MB/s]\n",
            " 53% 189M/358M [00:37<00:41, 4.10MB/s]\n",
            " 53% 191M/358M [00:38<00:32, 5.08MB/s]\n",
            " 54% 192M/358M [00:38<00:51, 3.22MB/s]\n",
            " 54% 193M/358M [00:38<00:40, 4.08MB/s]\n",
            " 55% 195M/358M [00:39<00:31, 5.09MB/s]\n",
            " 55% 196M/358M [00:39<00:50, 3.21MB/s]\n",
            " 55% 197M/358M [00:39<00:42, 3.81MB/s]\n",
            " 56% 199M/358M [00:40<00:32, 4.81MB/s]\n",
            " 56% 200M/358M [00:40<00:50, 3.14MB/s]\n",
            " 56% 201M/358M [00:40<00:42, 3.72MB/s]\n",
            " 57% 202M/358M [00:41<00:36, 4.27MB/s]\n",
            " 57% 203M/358M [00:41<00:32, 4.79MB/s]\n",
            " 57% 204M/358M [00:41<00:44, 3.43MB/s]\n",
            " 57% 205M/358M [00:41<00:42, 3.63MB/s]\n",
            " 57% 205M/358M [00:42<00:45, 3.33MB/s]\n",
            " 58% 207M/358M [00:42<00:32, 4.63MB/s]\n",
            " 58% 208M/358M [00:42<00:45, 3.31MB/s]\n",
            " 58% 209M/358M [00:42<00:40, 3.64MB/s]\n",
            " 59% 210M/358M [00:43<00:37, 3.94MB/s]\n",
            " 59% 211M/358M [00:43<00:35, 4.19MB/s]\n",
            " 59% 212M/358M [00:43<00:33, 4.42MB/s]\n",
            " 59% 212M/358M [00:43<00:38, 3.75MB/s]\n",
            " 59% 213M/358M [00:43<00:39, 3.67MB/s]\n",
            " 60% 214M/358M [00:44<00:35, 4.08MB/s]\n",
            " 60% 215M/358M [00:44<00:32, 4.40MB/s]\n",
            " 60% 216M/358M [00:44<00:40, 3.48MB/s]\n",
            " 61% 217M/358M [00:44<00:36, 3.89MB/s]\n",
            " 61% 218M/358M [00:45<00:32, 4.26MB/s]\n",
            " 61% 219M/358M [00:45<00:30, 4.55MB/s]\n",
            " 61% 220M/358M [00:45<00:40, 3.44MB/s]\n",
            " 62% 221M/358M [00:45<00:35, 3.88MB/s]\n",
            " 62% 222M/358M [00:46<00:31, 4.26MB/s]\n",
            " 62% 223M/358M [00:46<00:29, 4.57MB/s]\n",
            " 63% 224M/358M [00:46<00:27, 4.84MB/s]\n",
            " 63% 225M/358M [00:46<00:26, 5.03MB/s]\n",
            " 63% 226M/358M [00:46<00:25, 5.17MB/s]\n",
            " 64% 227M/358M [00:47<00:24, 5.28MB/s]\n",
            " 64% 228M/358M [00:47<00:24, 5.37MB/s]\n",
            " 64% 229M/358M [00:47<00:35, 3.65MB/s]\n",
            " 64% 230M/358M [00:47<00:31, 4.10MB/s]\n",
            " 65% 231M/358M [00:48<00:28, 4.50MB/s]\n",
            " 65% 232M/358M [00:48<00:26, 4.81MB/s]\n",
            " 65% 233M/358M [00:48<00:36, 3.44MB/s]\n",
            " 65% 234M/358M [00:48<00:31, 3.95MB/s]\n",
            " 66% 235M/358M [00:49<00:27, 4.38MB/s]\n",
            " 66% 236M/358M [00:49<00:25, 4.72MB/s]\n",
            " 66% 237M/358M [00:49<00:35, 3.35MB/s]\n",
            " 67% 238M/358M [00:49<00:30, 3.87MB/s]\n",
            " 67% 239M/358M [00:50<00:27, 4.34MB/s]\n",
            " 67% 240M/358M [00:50<00:24, 4.69MB/s]\n",
            " 67% 241M/358M [00:50<00:35, 3.32MB/s]\n",
            " 68% 242M/358M [00:50<00:29, 3.87MB/s]\n",
            " 68% 243M/358M [00:51<00:26, 4.31MB/s]\n",
            " 68% 244M/358M [00:51<00:24, 4.69MB/s]\n",
            " 69% 245M/358M [00:51<00:34, 3.30MB/s]\n",
            " 69% 246M/358M [00:51<00:28, 3.84MB/s]\n",
            " 69% 247M/358M [00:52<00:25, 4.30MB/s]\n",
            " 69% 248M/358M [00:52<00:23, 4.68MB/s]\n",
            " 70% 250M/358M [00:52<00:21, 4.96MB/s]\n",
            " 70% 251M/358M [00:52<00:20, 5.17MB/s]\n",
            " 70% 252M/358M [00:52<00:19, 5.34MB/s]\n",
            " 71% 253M/358M [00:53<00:19, 5.44MB/s]\n",
            " 71% 253M/358M [00:53<00:36, 2.87MB/s]\n",
            " 71% 255M/358M [00:53<00:29, 3.43MB/s]\n",
            " 72% 256M/358M [00:54<00:25, 3.95MB/s]\n",
            " 72% 257M/358M [00:54<00:22, 4.38MB/s]\n",
            " 72% 257M/358M [00:54<00:31, 3.20MB/s]\n",
            " 72% 259M/358M [00:54<00:26, 3.76MB/s]\n",
            " 73% 260M/358M [00:55<00:23, 4.22MB/s]\n",
            " 73% 261M/358M [00:55<00:20, 4.61MB/s]\n",
            " 73% 261M/358M [00:55<00:29, 3.30MB/s]\n",
            " 73% 263M/358M [00:55<00:24, 3.84MB/s]\n",
            " 74% 264M/358M [00:56<00:21, 4.32MB/s]\n",
            " 74% 265M/358M [00:56<00:19, 4.68MB/s]\n",
            " 74% 265M/358M [00:56<00:27, 3.30MB/s]\n",
            " 75% 267M/358M [00:56<00:23, 3.85MB/s]\n",
            " 75% 268M/358M [00:57<00:20, 4.32MB/s]\n",
            " 75% 269M/358M [00:57<00:18, 4.70MB/s]\n",
            " 75% 269M/358M [00:57<00:26, 3.29MB/s]\n",
            " 76% 271M/358M [00:57<00:22, 3.86MB/s]\n",
            " 76% 272M/358M [00:58<00:19, 4.33MB/s]\n",
            " 76% 273M/358M [00:58<00:17, 4.71MB/s]\n",
            " 76% 273M/358M [00:58<00:25, 3.28MB/s]\n",
            " 77% 275M/358M [00:58<00:21, 3.85MB/s]\n",
            " 77% 276M/358M [00:59<00:18, 4.34MB/s]\n",
            " 77% 277M/358M [00:59<00:17, 4.75MB/s]\n",
            " 78% 277M/358M [00:59<00:24, 3.29MB/s]\n",
            " 78% 279M/358M [00:59<00:20, 3.88MB/s]\n",
            " 78% 280M/358M [01:00<00:17, 4.38MB/s]\n",
            " 79% 281M/358M [01:00<00:15, 4.81MB/s]\n",
            " 79% 281M/358M [01:00<00:23, 3.27MB/s]\n",
            " 79% 283M/358M [01:00<00:19, 3.85MB/s]\n",
            " 79% 284M/358M [01:01<00:16, 4.39MB/s]\n",
            " 80% 285M/358M [01:01<00:15, 4.84MB/s]\n",
            " 80% 286M/358M [01:01<00:13, 5.18MB/s]\n",
            " 80% 287M/358M [01:01<00:12, 5.46MB/s]\n",
            " 81% 289M/358M [01:01<00:12, 5.67MB/s]\n",
            " 81% 290M/358M [01:02<00:11, 5.85MB/s]\n",
            " 81% 291M/358M [01:02<00:11, 5.99MB/s]\n",
            " 82% 292M/358M [01:02<00:17, 3.76MB/s]\n",
            " 82% 293M/358M [01:02<00:16, 4.04MB/s]\n",
            " 82% 294M/358M [01:03<00:13, 4.66MB/s]\n",
            " 83% 295M/358M [01:03<00:12, 5.16MB/s]\n",
            " 83% 297M/358M [01:03<00:10, 5.57MB/s]\n",
            " 83% 298M/358M [01:03<00:10, 5.86MB/s]\n",
            " 84% 299M/358M [01:03<00:09, 6.14MB/s]\n",
            " 84% 301M/358M [01:04<00:08, 6.35MB/s]\n",
            " 84% 302M/358M [01:04<00:08, 6.53MB/s]\n",
            " 85% 303M/358M [01:04<00:13, 4.15MB/s]\n",
            " 85% 303M/358M [01:04<00:13, 4.06MB/s]\n",
            " 85% 305M/358M [01:05<00:10, 4.84MB/s]\n",
            " 86% 306M/358M [01:05<00:15, 3.34MB/s]\n",
            " 86% 307M/358M [01:05<00:12, 4.13MB/s]\n",
            " 86% 309M/358M [01:06<00:10, 4.85MB/s]\n",
            " 87% 310M/358M [01:06<00:14, 3.39MB/s]\n",
            " 87% 311M/358M [01:06<00:11, 4.16MB/s]\n",
            " 88% 313M/358M [01:07<00:09, 4.92MB/s]\n",
            " 88% 314M/358M [01:07<00:13, 3.33MB/s]\n",
            " 88% 315M/358M [01:07<00:10, 4.15MB/s]\n",
            " 89% 317M/358M [01:08<00:08, 4.93MB/s]\n",
            " 89% 318M/358M [01:08<00:12, 3.29MB/s]\n",
            " 89% 320M/358M [01:08<00:09, 4.16MB/s]\n",
            " 90% 321M/358M [01:09<00:07, 5.00MB/s]\n",
            " 90% 322M/358M [01:09<00:11, 3.23MB/s]\n",
            " 90% 324M/358M [01:09<00:08, 4.16MB/s]\n",
            " 91% 325M/358M [01:10<00:06, 5.06MB/s]\n",
            " 91% 326M/358M [01:10<00:09, 3.21MB/s]\n",
            " 92% 328M/358M [01:10<00:07, 4.19MB/s]\n",
            " 92% 329M/358M [01:11<00:05, 5.17MB/s]\n",
            " 92% 330M/358M [01:11<00:08, 3.26MB/s]\n",
            " 93% 332M/358M [01:11<00:06, 4.15MB/s]\n",
            " 93% 334M/358M [01:12<00:04, 5.21MB/s]\n",
            " 93% 334M/358M [01:12<00:07, 3.28MB/s]\n",
            " 94% 336M/358M [01:12<00:05, 4.09MB/s]\n",
            " 94% 338M/358M [01:13<00:03, 5.23MB/s]\n",
            " 95% 338M/358M [01:13<00:05, 3.30MB/s]\n",
            " 95% 340M/358M [01:13<00:04, 4.05MB/s]\n",
            " 96% 342M/358M [01:14<00:03, 5.14MB/s]\n",
            " 96% 342M/358M [01:14<00:04, 3.28MB/s]\n",
            " 96% 343M/358M [01:14<00:04, 3.52MB/s]\n",
            " 96% 345M/358M [01:15<00:02, 4.34MB/s]\n",
            " 97% 346M/358M [01:15<00:03, 3.45MB/s]\n",
            " 97% 348M/358M [01:15<00:02, 4.23MB/s]\n",
            " 98% 349M/358M [01:16<00:01, 4.99MB/s]\n",
            " 98% 351M/358M [01:16<00:01, 3.64MB/s]\n",
            " 98% 352M/358M [01:16<00:01, 3.91MB/s]\n",
            " 99% 353M/358M [01:17<00:00, 4.81MB/s]\n",
            " 99% 354M/358M [01:17<00:00, 5.02MB/s]\n",
            " 99% 355M/358M [01:17<00:00, 3.58MB/s]\n",
            "100% 356M/358M [01:17<00:00, 3.67MB/s]\n",
            "100% 357M/358M [01:18<00:00, 4.22MB/s]\n",
            "43650it [01:18, 558.43it/s]\n",
            "100% 358M/358M [01:18<00:00, 4.57MB/s]\n",
            "Extracting archive...\n",
            "Detected database version: 1.0.0\n",
            "Migrating\n",
            "Dataset now on version 1.0.0.\n"
          ]
        }
      ],
      "source": [
        "!shapesd download --ckpturl \"https://lmaytie.com/tutorial/simple_shapes_100k.tar.gz\" --name \"simple_shapes_100k\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "94faa6c3",
      "metadata": {
        "id": "94faa6c3"
      },
      "source": [
        "The different modalities composing the dataset can be aligned (paired) or not. For example a generated text can describe an image or be unrelated to any as shown here:\n",
        "\n",
        "<img src=\"https://lmaytie.com/tutorial/paired.png?\" width=\"500\"/>\n",
        "\n",
        "We need to define how many samples in the dataset are aligned across all modalities. In this case, we are using fully paired data (each sample includes an image, its corresponding attributes, and a descriptive text).\n",
        "\n",
        "However, it's also possible to train with only a subset of fully paired samples, while the rest of the dataset includes unpaired data from individual modalities (e.g., many images, texts, and attribute sets without direct links). This setup enables leveraging large amounts of unimodal data while still learning cross-modal relationships, thanks to cycle-consistency losses, which we will discuss later."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "1264f6d2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1264f6d2",
        "outputId": "e1647e99-9ee5-413a-e219-b78aaf02ab20"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-07-22 11:34:14.550089: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1753184054.570186    4349 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1753184054.576266    4349 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
          ]
        }
      ],
      "source": [
        "!shapesd alignment add --dataset_path \"simple_shapes_100k\" --seed 0 --domain_alignment attr,t,v 1.0"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "41bf1e6f",
      "metadata": {
        "id": "41bf1e6f"
      },
      "source": [
        "# Training a Global Workspace"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "852d64e9",
      "metadata": {
        "id": "852d64e9"
      },
      "source": [
        "Now that we have a dataset let go through the Global Workspace model used in this tutorial"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "731e663d",
      "metadata": {
        "id": "731e663d"
      },
      "source": [
        "\n",
        "The code used in this tutorial is split across three repositories, each with a specific role:\n",
        "\n",
        "1. **[simple-shapes-dataset](https://github.com/ruflab/simple-shapes-dataset)**  \n",
        "   Implements the *Simple Shapes* dataset described earlier.  \n",
        "   * This is the **data source** used in our experiments.\n",
        "\n",
        "2. **[shimmer](https://github.com/ruflab/shimmer)**  \n",
        "   Implements the **core Global Workspace architecture**, designed to be modular and reusable across multiple datasets.  \n",
        "   * This is the **framework** that defines the common components of the model.\n",
        "\n",
        "3. **[shimmer-ssd](https://github.com/ruflab/shimmer-ssd)**  \n",
        "   Specializes the Global Workspace model for the *Simple Shapes* dataset by integrating it with `simple-shapes-dataset` and extending the base functionality from `shimmer`.  \n",
        "   * This is the **task-specific implementation** built on top of the other two.\n",
        "\n",
        "\n",
        "The Global Workspace is defined as a pytorch lightning module named **GlobalWorkspace**. It is composed of different elements:\n",
        "* Pre-trained unimodal models to encode raw high dimensional data into unimodal latent vectors. These models are defined as **DomainModule**\n",
        "* Encoders and Decoders as well as losses and coefficients to train them. All of these are defined inside **GWModule**\n",
        "\n",
        "<img src=\"https://lmaytie.com/tutorial/shimmer_architecture.png\" width=\"1500\"/>\n",
        "\n",
        "GlobalWorkspace Classes\n",
        "* DomainModule: Unimodal model for each modality\n",
        "* GWModule: Encoders and Decoders composing the Global Workspace model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e4aca17d",
      "metadata": {
        "id": "e4aca17d"
      },
      "source": [
        "## Unimodal Domains"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f8d47139",
      "metadata": {
        "id": "f8d47139"
      },
      "source": [
        "Let first go through the **DomainModules**. Their role is to encode raw data into latent vectors. As we have three different modalities here we are going to define one module for each modality.\n",
        "\n",
        "No training is required here, as we will load pre-trained models to focus on learning within the Global Workspace."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "99c33b94",
      "metadata": {
        "id": "99c33b94"
      },
      "source": [
        "**DomainModule** requires a set of function common to all modalities to work:\n",
        "* `def encode(self, x: Any) -> torch.Tensor`: encodes raw data into unimodal latent representation\n",
        "* `def decode(self, z: torch.Tensor) -> Any`: decoded unimodal latent representation into raw data\n",
        "* `def compute_loss(self, pred: torch.Tensor, target: torch.Tensor, raw_target: Any) -> LossOutput | None`: The loss used for this domain by the Global Workspace to compute cycle consitency or translation. `pred` is the predicted unimodal latent vector coming from the Global Workspace, `target` is the target latent vector and `raw_target` is the original input before being encoded.\n",
        "\n",
        "In addition classical fucntions of Pytorch Lightning must be implemented: `configure_optimizers`, `training_step`, and `validation_step`. See the [lightning docs](https://lightning.ai/docs/pytorch/stable/common/lightning_module.html) for more details."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "58247610",
      "metadata": {
        "id": "58247610"
      },
      "source": [
        "### Vision"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cfb129f7",
      "metadata": {
        "id": "cfb129f7"
      },
      "source": [
        "The visual domain module is based on a Variational Autoencoder (VAE) architecture. It encodes an input image into a unimodal latent vector using the reparameterization trick during training. This trick allows the model to sample from a Gaussian distribution in a way that is compatible with gradient-based optimization, making the latent space more suitable for generating new images.\n",
        "\n",
        "At inference time, the model skips sampling and simply uses the mean of the predicted distribution as the latent representation.\n",
        "\n",
        "The decoder then takes this latent vector and attempts to reconstruct the original image.\n",
        "\n",
        "The model is trained with two objectives:\n",
        "* Latent regularization: Encourage the latent space to follow a standard Gaussian distribution using a Kullback–Leibler (KL) divergence loss between the predicted distribution (mean and variance) and a standard normal distribution.\n",
        "* Reconstruction: Ensure that the decoder can accurately reconstruct the input image by minimizing a reconstruction loss (e.g., mean squared error or binary cross-entropy) between the predicted image and the ground truth.\n",
        "\n",
        "<img src=\"https://lmaytie.com/tutorial/Uni_V.png\" width=\"1000\"/>\n",
        "\n",
        "In this project, the vision module is implemented in the class called `VisualDomainModule`.\n",
        "\n",
        "Once the VAE is trained, it becomes computationally expensive to run the encoder (a large CNN) on every image during the training of the Global Workspace. To avoid this overhead, we use a simple but effective trick:\n",
        "we precompute and save the visual latent vectors for all images in the dataset as .npy files. This allows us to bypass the encoder during Global Workspace training and significantly speed up the process.\n",
        "\n",
        "To support this setup, we define a second class called `VisualLatentDomainModule`. This class takes precomputed visual latent vectors as input and adapts the `encode()` and `decode()` methods accordingly. In other words, instead of computing latents from raw images, it directly works with the stored latent representations."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6046adb7",
      "metadata": {
        "id": "6046adb7"
      },
      "source": [
        "### Attributes"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "142a4ab7",
      "metadata": {
        "id": "142a4ab7"
      },
      "source": [
        "The attributes module is very similar to the visual module. It is also implemented as a VAE, but instead of processing images, it takes attribute vectors as input. It is defined as `AttributesDomainModule`\n",
        "\n",
        "Aside from the input format, the architecture and training process remain the same:\n",
        "the encoder maps the attribute vector to a latent distribution, and the decoder reconstructs the original vector from a sampled latent code. The model is trained using both reconstruction loss and KL divergence, just like in the visual case.\n",
        "\n",
        "<img src=\"https://lmaytie.com/tutorial/Uni_Attr.png?v=1234\" width=\"1000\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c105d481",
      "metadata": {
        "id": "c105d481"
      },
      "source": [
        "### Text"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e64d8538",
      "metadata": {
        "id": "e64d8538"
      },
      "source": [
        "The text module, implemented as `GRUTextDomainModule`, is a bit more complex than the visual and attributes modules. It consists of three main components:\n",
        "\n",
        "**Encoder**:\n",
        "* **Tokenizer and Embedding Layer**: Converts raw natural language text into token vectors.\n",
        "* **BERT Encoder**: Processes the token sequence and extracts the [CLS] token, which serves as a fixed-size representation summarizing the sentence.\n",
        "* **VAE Encoder**: Projects the high-dimensional [CLS] embedding into a lower-dimensional latent space using a variational encoder.\n",
        "\n",
        "**Decoder**:\n",
        "* **VAE Decoder**: Reconstructs the [CLS] token embedding from the latent vector.\n",
        "* **GRU Decoder**: Generates the original sentence autoregressively from the reconstructed latent representation.\n",
        "\n",
        "**Training Objectives**:\n",
        "* **VAE Losses**: Encourage the latent space to follow a Gaussian distribution using KL divergence and reconstruction of the [CLS] embedding.\n",
        "* **Sentence Reconstruction Loss**: Trains the GRU decoder to regenerate the original sentence as closely as possible from the latent vector.\n",
        "\n",
        "<img src=\"https://lmaytie.com/tutorial/Uni_text.png\" width=\"1000\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c8c17dfb",
      "metadata": {
        "id": "c8c17dfb"
      },
      "source": [
        "## Global Workspace"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4b28f594",
      "metadata": {
        "id": "4b28f594"
      },
      "source": [
        "### Theory"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "499fcf07",
      "metadata": {
        "id": "499fcf07"
      },
      "source": [
        "#### Global Workspace Core Architecture\n",
        "\n",
        "The Global Workspace model integrates multiple pre-trained unimodal models and a set of encoders decoders to form a unified representation. Let's illustrate how data go through the model taking the Figure bellow as an example.\n",
        "\n",
        "We start from raw observations:\n",
        "- Visual input: $o^v$\n",
        "- Textual input: $o^t$\n",
        "\n",
        "Each of these is passed through a **pretrained unimodal encoder**:\n",
        "- A VAE (for vision)\n",
        "- A BERT-Encoder model (for text)\n",
        "\n",
        "These produce **unimodal latent representations**:\n",
        "- $u^v$ from visual input\n",
        "- $u^t$ from textual input\n",
        "\n",
        "Each unimodal latent vector is then encoded into a **pre-fusion Global Workspace vector**:\n",
        "- $z^v = e_v(u^v)$\n",
        "- $z^t = e_t(u^t)$  \n",
        "where $e_v$ and $e_t$ are modality-specific MLP encoders.\n",
        "\n",
        "The two pre-fusion vectors are then fused into a single **Global Workspace representation** $z$ using a weighted sum:\n",
        "\n",
        "$$\n",
        "z = \\alpha_v \\cdot z^v + \\alpha_t \\cdot z^t\n",
        "$$\n",
        "\n",
        "Here, $\\alpha_v$ and $\\alpha_t$ are modality-specific attention weights that determine how much each modality contributes to the final shared representation.\n",
        "\n",
        "Finally, this shared representation is decoded back into unimodal latent spaces:\n",
        "- $\\hat{u}^v$ for the vision modality\n",
        "- $\\hat{u}^t$ for the text modality\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img src=\"https://lmaytie.com/tutorial/GW_fusion_t_v.png\" width=\"1000\"/>\n",
        "</p>\n",
        "\n",
        "\n",
        "#### Loss Functions\n",
        "\n",
        "The model is trained using multiple loss functions, each encouraging a different type of **cross-modal interaction**. These are shown in the figure below:\n",
        "\n",
        "##### 1. Contrastive Loss ($\\mathcal{L}_{\\text{cont}}$)\n",
        "\n",
        "- Applied between pre-fusion vectors ($z^v$, $z^t$)\n",
        "- Encourages **alignment** of representations before fusion\n",
        "- Logged as: `contrastive_A_and_B`\n",
        "\n",
        "\n",
        "##### 2. Broadcast Loss ($\\mathcal{L}_{\\text{borad}}$)\n",
        "\n",
        "Encourages **cross-modal decoding** through the Global Workspace. This consists of several sub-losses:\n",
        "\n",
        "- **Translation Loss** ($\\mathcal{L}_{\\text{tr}}$):  \n",
        "  Translate from one modality to another via the workspace  \n",
        "  → Requires **paired data**  \n",
        "  → Logged as: `From A to B`\n",
        "\n",
        "- **Demi-Cycle Loss** ($\\mathcal{L}_{\\text{dcy}}$):  \n",
        "  Encode and decode within the **same modality** through the workspace  \n",
        "  → Can use **unpaired data**  \n",
        "  → Logged as: `From A to A`\n",
        "\n",
        "- **Cycle Loss** ($\\mathcal{L}_{\\text{cy}}$):  \n",
        "  Translate to another modality, then back to the original  \n",
        "  → Can use **unpaired data**  \n",
        "  → Logged as: `From A through B to A` or `From A through {B,C} to A`\n",
        "\n",
        "- **Fusion Loss** ($\\mathcal{L}_{\\text{fusion}}$):  \n",
        "  Fuse multiple modalities before decoding  \n",
        "  → Requires **paired data**  \n",
        "  → Logged as:  \n",
        "    - `From {A,B,C} to A`  \n",
        "    - `From {A,B} to C`  \n",
        "    - `From {A,B} to {A,B,C}`\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img src=\"https://lmaytie.com/tutorial/losses.png?\" width=\"1000\"/>\n",
        "</p>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0f6bba16",
      "metadata": {
        "id": "0f6bba16"
      },
      "source": [
        "### Practice"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "64a2ec6a",
      "metadata": {
        "id": "64a2ec6a"
      },
      "source": [
        "Let's first generate the `config` folder, which will contain the YAML files used by the other scripts and this notebook"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "60c692b3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "60c692b3",
        "outputId": "17e3488f-1fbf-466f-d1fd-fd458e98fe8a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-07-22 11:58:07.901996: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1753185487.922329   10675 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1753185487.928500   10675 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "Config folder already exists. Skipping.\n"
          ]
        }
      ],
      "source": [
        "!ssd config create"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "379864fb",
      "metadata": {
        "id": "379864fb"
      },
      "source": [
        "We have to update `main.yaml` file. It has to be done manually by oppening the file and copy pasting the content bellow\n",
        "\n",
        "First, let's use the dataset we downloaded earlier\n",
        "```yaml\n",
        "dataset:\n",
        "    path: \"./simple_shapes_100k\" # Path to the simple-shapes-dataset\n",
        "```\n",
        "\n",
        "let's reduce the batch size to be able to run it on google colab\n",
        "```yaml\n",
        "training:\n",
        "    batch_size: 128\n",
        "    max_steps: 5000\n",
        "```\n",
        "\n",
        "let's update the alignment split to get the same we defined at the beginning:\n",
        "```yaml\n",
        "domain_proportions:\n",
        "    -   domains: [\"v\", \"t\", \"attr\"]  # vision, attributes and text paired passes uses 100% of the available data\n",
        "        proportion: 1.0\n",
        "```\n",
        "\n",
        "let's change the selected domains and their checkpoint:\n",
        "\n",
        "```yaml\n",
        "domains:\n",
        "    - checkpoint_path: \"./checkpoints/domain_v.ckpt\"    # visual module checkpoint\n",
        "      domain_type: v_latents\n",
        "    - checkpoint_path: \"./checkpoints/domain_attr.ckpt\" # attributes module checkpoint\n",
        "      domain_type: attr\n",
        "    - checkpoint_path: \"./checkpoints/domain_t.ckpt\"    # text module checkpoint\n",
        "      domain_type: t\n",
        "```\n",
        "\n",
        "and let's define the global workspace dimenison to 16 and the coefficient of each loss:\n",
        "```yaml\n",
        "global_workspace:\n",
        "    latent_dim: 16  \n",
        "    \n",
        "    loss_coefficients:\n",
        "        cycles: 1.0\n",
        "        contrastives: 0.1\n",
        "        demi_cycles: 1.0\n",
        "        translations: 1.0\n",
        "\n",
        "    encoders:\n",
        "        hidden_dim: 32\n",
        "        n_layers: 3\n",
        "\n",
        "    decoders:\n",
        "        hidden_dim: 32\n",
        "        n_layers: 3\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "85acbf54",
      "metadata": {
        "id": "85acbf54"
      },
      "source": [
        "A lot of files are genrated by this command. For this tutorial we are going to use two of them: `main.yaml` and `train_gw.yaml`\n",
        "\n",
        "<div class=\"alert alert-info\">\n",
        "Anytime you make a change to the config, don't forget to reload it with the following cell!\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "96396615",
      "metadata": {
        "id": "96396615",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "outputId": "11cbb830-0e70-4131-a167-940f10d2e2cd"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ScannerError",
          "evalue": "while scanning a simple key\n  in \"config/main.yaml\", line 18, column 1\ncould not find expected ':'\n  in \"config/main.yaml\", line 19, column 1",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mScannerError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-18-620092392.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./config\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_cli\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mload_files\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"train_gw.yaml\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/shimmer_ssd/config.py\u001b[0m in \u001b[0;36mload_config\u001b[0;34m(path, load_files, use_cli, debug_mode, argv, log_config)\u001b[0m\n\u001b[1;32m    522\u001b[0m         \u001b[0mconf_files\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"debug.yaml\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 524\u001b[0;31m     \u001b[0mconfig_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcli_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_config_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconf_files\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_cli\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    525\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    526\u001b[0m     config_dict.update(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/cfg_tools/utils.py\u001b[0m in \u001b[0;36mload_config_files\u001b[0;34m(path, load_files, use_cli, argv)\u001b[0m\n\u001b[1;32m     73\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Config file {path_file} does not exist.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_file\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m             \u001b[0mmerge_dicts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myaml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msafe_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[0mcli_config\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/yaml/__init__.py\u001b[0m in \u001b[0;36msafe_load\u001b[0;34m(stream)\u001b[0m\n\u001b[1;32m    123\u001b[0m     \u001b[0mto\u001b[0m \u001b[0mbe\u001b[0m \u001b[0msafe\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0muntrusted\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m     \"\"\"\n\u001b[0;32m--> 125\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSafeLoader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msafe_load_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/yaml/__init__.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(stream, Loader)\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0mloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_single_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0mloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/yaml/constructor.py\u001b[0m in \u001b[0;36mget_single_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_single_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;31m# Ensure that the stream contains a single document and construct it.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m         \u001b[0mnode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_single_node\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnode\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstruct_document\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/yaml/composer.py\u001b[0m in \u001b[0;36mget_single_node\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mdocument\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mStreamEndEvent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m             \u001b[0mdocument\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompose_document\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0;31m# Ensure that the stream contains no more documents.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/yaml/composer.py\u001b[0m in \u001b[0;36mcompose_document\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0;31m# Compose the root node.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0mnode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompose_node\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0;31m# Drop the DOCUMENT-END event.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/yaml/composer.py\u001b[0m in \u001b[0;36mcompose_node\u001b[0;34m(self, parent, index)\u001b[0m\n\u001b[1;32m     82\u001b[0m             \u001b[0mnode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompose_sequence_node\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manchor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMappingStartEvent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m             \u001b[0mnode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompose_mapping_node\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manchor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mascend_resolver\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/yaml/composer.py\u001b[0m in \u001b[0;36mcompose_mapping_node\u001b[0;34m(self, anchor)\u001b[0m\n\u001b[1;32m    131\u001b[0m             \u001b[0;31m#    raise ComposerError(\"while composing a mapping\", start_event.start_mark,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m             \u001b[0;31m#            \"found duplicate key\", key_event.start_mark)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m             \u001b[0mitem_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompose_node\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitem_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    134\u001b[0m             \u001b[0;31m#node.value[item_key] = item_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m             \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem_key\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitem_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/yaml/composer.py\u001b[0m in \u001b[0;36mcompose_node\u001b[0;34m(self, parent, index)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcompose_node\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAliasEvent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m             \u001b[0mevent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m             \u001b[0manchor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0manchor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/yaml/parser.py\u001b[0m in \u001b[0;36mcheck_event\u001b[0;34m(self, *choices)\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_event\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_event\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_event\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mchoices\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/yaml/parser.py\u001b[0m in \u001b[0;36mparse_block_mapping_value\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    447\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_token\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mValueToken\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m             \u001b[0mtoken\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_token\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 449\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_token\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mKeyToken\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueToken\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBlockEndToken\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    450\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_block_mapping_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_block_node_or_indentless_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/yaml/scanner.py\u001b[0m in \u001b[0;36mcheck_token\u001b[0;34m(self, *choices)\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcheck_token\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mchoices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;31m# Check if the next token is one of the given types.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m         \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mneed_more_tokens\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch_more_tokens\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/yaml/scanner.py\u001b[0m in \u001b[0;36mneed_more_tokens\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    150\u001b[0m         \u001b[0;31m# The current token may be a potential simple key, so we\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m         \u001b[0;31m# need to look further.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 152\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstale_possible_simple_keys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    153\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_possible_simple_key\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokens_taken\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/yaml/scanner.py\u001b[0m in \u001b[0;36mstale_possible_simple_keys\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    289\u001b[0m                     \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1024\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequired\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 291\u001b[0;31m                     raise ScannerError(\"while scanning a simple key\", key.mark,\n\u001b[0m\u001b[1;32m    292\u001b[0m                             \"could not find expected ':'\", self.get_mark())\n\u001b[1;32m    293\u001b[0m                 \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpossible_simple_keys\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mScannerError\u001b[0m: while scanning a simple key\n  in \"config/main.yaml\", line 18, column 1\ncould not find expected ':'\n  in \"config/main.yaml\", line 19, column 1"
          ]
        }
      ],
      "source": [
        "config = load_config(\"./config\", use_cli=False, load_files=[\"train_gw.yaml\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0ac97858",
      "metadata": {
        "id": "0ac97858"
      },
      "source": [
        "This tutorial also require the simple-shapes dataset tokenizer data, so let's download this from the `shimmer-ssd` repository:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "c60da5aa",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c60da5aa",
        "outputId": "69f92c28-fa5f-45f2-db1b-4de1b6d18ca9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-07-22 12:05:31.697687: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1753185931.718663   12555 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1753185931.724971   12555 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "Downloading in tokenizer.\n",
            "Downloading in tokenizer.\n",
            "Tokenizer path already exists. Skipping.\n"
          ]
        }
      ],
      "source": [
        "!ssd download tokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "99eaa9dd",
      "metadata": {
        "id": "99eaa9dd"
      },
      "source": [
        "Let's skip the training of these unimodal modules and download checkpoints from pre-trained models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "d5899c6c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d5899c6c",
        "outputId": "41a1deca-b856-42fb-a0b2-45e1375f6660"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-07-22 11:40:53.146756: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1753184453.169071    6053 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1753184453.176059    6053 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "Downloading in checkpoints.\n",
            "  0% 0.00/53.5M [00:00<?, ?B/s]\n",
            "  0% 8.19k/53.5M [00:00<20:59, 42.4kB/s]\n",
            "  0% 41.0k/53.5M [00:00<07:38, 117kB/s] \n",
            "  0% 73.7k/53.5M [00:00<06:20, 140kB/s]\n",
            "  0% 164k/53.5M [00:00<03:19, 268kB/s] \n",
            "  1% 279k/53.5M [00:00<02:18, 384kB/s]\n",
            "  1% 573k/53.5M [00:01<01:08, 770kB/s]\n",
            "  2% 1.18M/53.5M [00:01<00:33, 1.54MB/s]\n",
            "  4% 2.31M/53.5M [00:01<00:17, 2.88MB/s]\n",
            "  6% 3.42M/53.5M [00:01<00:13, 3.78MB/s]\n",
            "  7% 3.81M/53.5M [00:01<00:15, 3.23MB/s]\n",
            " 12% 6.39M/53.5M [00:02<00:07, 6.31MB/s]\n",
            " 14% 7.63M/53.5M [00:02<00:07, 6.29MB/s]\n",
            " 15% 8.27M/53.5M [00:02<00:08, 5.38MB/s]\n",
            " 19% 10.2M/53.5M [00:02<00:06, 6.78MB/s]\n",
            " 21% 11.4M/53.5M [00:02<00:06, 6.58MB/s]\n",
            " 24% 12.6M/53.5M [00:03<00:06, 6.48MB/s]\n",
            " 26% 13.8M/53.5M [00:03<00:06, 6.43MB/s]\n",
            " 28% 15.1M/53.5M [00:03<00:05, 6.42MB/s]\n",
            " 31% 16.3M/53.5M [00:03<00:05, 6.43MB/s]\n",
            " 33% 17.6M/53.5M [00:03<00:05, 6.49MB/s]\n",
            " 35% 18.9M/53.5M [00:04<00:05, 6.52MB/s]\n",
            " 38% 20.2M/53.5M [00:04<00:05, 6.58MB/s]\n",
            " 40% 21.5M/53.5M [00:04<00:04, 6.62MB/s]\n",
            " 43% 22.8M/53.5M [00:04<00:04, 6.69MB/s]\n",
            " 45% 24.2M/53.5M [00:04<00:04, 6.74MB/s]\n",
            " 48% 25.5M/53.5M [00:05<00:04, 6.81MB/s]\n",
            " 50% 26.9M/53.5M [00:05<00:03, 6.86MB/s]\n",
            " 53% 28.2M/53.5M [00:05<00:03, 6.88MB/s]\n",
            " 55% 29.6M/53.5M [00:05<00:03, 6.94MB/s]\n",
            " 58% 31.0M/53.5M [00:05<00:03, 6.99MB/s]\n",
            " 61% 32.4M/53.5M [00:06<00:02, 7.03MB/s]\n",
            " 63% 33.8M/53.5M [00:06<00:02, 7.07MB/s]\n",
            " 66% 35.2M/53.5M [00:06<00:02, 7.09MB/s]\n",
            " 68% 36.6M/53.5M [00:06<00:02, 7.14MB/s]\n",
            " 71% 38.0M/53.5M [00:06<00:02, 7.18MB/s]\n",
            " 74% 39.4M/53.5M [00:07<00:01, 7.18MB/s]\n",
            " 76% 40.8M/53.5M [00:07<00:01, 7.20MB/s]\n",
            " 79% 42.2M/53.5M [00:07<00:01, 7.21MB/s]\n",
            " 82% 43.6M/53.5M [00:07<00:01, 7.31MB/s]\n",
            " 84% 45.1M/53.5M [00:07<00:01, 7.32MB/s]\n",
            " 87% 46.5M/53.5M [00:07<00:00, 7.32MB/s]\n",
            " 90% 47.9M/53.5M [00:08<00:00, 7.32MB/s]\n",
            " 92% 49.3M/53.5M [00:08<00:00, 7.33MB/s]\n",
            " 95% 50.8M/53.5M [00:08<00:00, 7.36MB/s]\n",
            " 98% 52.2M/53.5M [00:08<00:00, 7.30MB/s]\n",
            "6525it [00:08, 744.22it/s]\n",
            "100% 53.5M/53.5M [00:08<00:00, 6.10MB/s]\n",
            "Extracting archive...\n",
            "2025-07-22 11:41:14.435087: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1753184474.455004    6152 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1753184474.461031    6152 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Saving train.\n",
            "100% 49/49 [00:39<00:00,  1.25it/s]\n",
            "Saving in simple_shapes_100k/saved_latents/train/domain_v.npy.\n",
            "Saving val.\n",
            "100% 1/1 [00:01<00:00,  1.83s/it]\n",
            "Saving in simple_shapes_100k/saved_latents/val/domain_v.npy.\n",
            "Saving test.\n",
            "100% 1/1 [00:01<00:00,  1.43s/it]\n",
            "Saving in simple_shapes_100k/saved_latents/test/domain_v.npy.\n"
          ]
        }
      ],
      "source": [
        "# Download checkpoints\n",
        "!ssd download checkpoints --ckpturl \"https://lmaytie.com/tutorial/tutorial_simple_shapes_models_ckpt.tar.gz\"\n",
        "\n",
        "# Extract visual latent from pretrained visual domain\n",
        "!ssd extract v \"checkpoints/domain_v.ckpt\" -p \"simple_shapes_100k\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8b2b9372",
      "metadata": {
        "id": "8b2b9372"
      },
      "source": [
        "We can now load pretrained unimodal modules"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "04803765",
      "metadata": {
        "id": "04803765"
      },
      "outputs": [],
      "source": [
        "domain_modules, gw_encoders, gw_decoders = load_pretrained_domains(\n",
        "    config.domains,\n",
        "    config.global_workspace.latent_dim,\n",
        "    config.global_workspace.encoders.hidden_dim,\n",
        "    config.global_workspace.encoders.n_layers,\n",
        "    config.global_workspace.decoders.hidden_dim,\n",
        "    config.global_workspace.decoders.n_layers,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d8d93f7a",
      "metadata": {
        "id": "d8d93f7a"
      },
      "source": [
        "And instanciate the Global Workspace class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "cb33fdc5",
      "metadata": {
        "id": "cb33fdc5"
      },
      "outputs": [],
      "source": [
        "def get_scheduler(optimizer: Optimizer) -> OneCycleLR:\n",
        "    return OneCycleLR(optimizer, config.training.optim.max_lr, config.training.max_steps)\n",
        "\n",
        "\n",
        "global_workspace = GlobalWorkspaceFusion(\n",
        "    domain_modules,\n",
        "    gw_encoders,\n",
        "    gw_decoders,\n",
        "    config.global_workspace.latent_dim,\n",
        "    config.global_workspace.loss_coefficients,\n",
        "    config.training.optim.lr,\n",
        "    config.training.optim.weight_decay,\n",
        "    scheduler=get_scheduler,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cf158610",
      "metadata": {
        "id": "cf158610"
      },
      "source": [
        "As we are working with pytorch Lightning we need to define a **DataModule**. It will handle the different Datasets and DataLoaders for us. For more details about **DataModule** please refer to [lightning docs](https://lightning.ai/docs/pytorch/stable/api/lightning.pytorch.core.LightningDataModule.html#lightning.pytorch.core.LightningDataModule)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "fb943c3d",
      "metadata": {
        "id": "fb943c3d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "outputId": "ffd6fa84-6901-4230-db07-147699f1fb47"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Domains set in `domain_classes` ({'v', 'attr', 't'}) are different from domains set in `domain_proportions` ({'v', 't'}).",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-11-507317789.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m ]\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m data_module = SimpleShapesDataModule(\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mdomain_classes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/simple_shapes_dataset/data_module.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, dataset_path, domain_classes, domain_proportions, batch_size, max_train_size, num_workers, seed, ood_seed, domain_args, additional_transforms, train_transforms, val_transforms, collate_fn, use_default_transforms)\u001b[0m\n\u001b[1;32m     55\u001b[0m         }\n\u001b[1;32m     56\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdomains_from_props\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mdomains_from_domain_classes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m     58\u001b[0m                 \u001b[0;34mf\"Domains set in `domain_classes` ({domains_from_domain_classes}) \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m                 \u001b[0;34m\"are different from domains set in \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Domains set in `domain_classes` ({'v', 'attr', 't'}) are different from domains set in `domain_proportions` ({'v', 't'})."
          ]
        }
      ],
      "source": [
        "domain_classes = get_default_domains([\"v_latents\", \"attr\", \"t\"])\n",
        "\n",
        "additional_transforms = {}\n",
        "additional_transforms[\"t\"] = [\n",
        "    TokenizeCaptions(\n",
        "        config.domain_modules.text.vocab_path,\n",
        "        config.domain_modules.text.merges_path,\n",
        "        config.domain_modules.text.seq_length,\n",
        "    )\n",
        "]\n",
        "\n",
        "data_module = SimpleShapesDataModule(\n",
        "        config.dataset.path,\n",
        "        domain_classes,\n",
        "        config.domain_proportions,\n",
        "        batch_size=config.training.batch_size,\n",
        "        num_workers=config.training.num_workers,\n",
        "        seed=config.seed,\n",
        "        ood_seed=config.ood_seed,\n",
        "        domain_args=config.domain_data_args,\n",
        "        additional_transforms=additional_transforms,\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a483fad9",
      "metadata": {
        "id": "a483fad9"
      },
      "source": [
        "We are going to use tensorboard to follow the training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ec0dbc78",
      "metadata": {
        "id": "ec0dbc78"
      },
      "outputs": [],
      "source": [
        "logger = TensorBoardLogger(\"logs\", name=\"gw\")\n",
        "\n",
        "# Get some image samples to log in tensorboard.\n",
        "train_samples = data_module.get_samples(\"train\", 32)\n",
        "val_samples = data_module.get_samples(\"val\", 32)\n",
        "\n",
        "# split the unique group in validation into individual groups for logging\n",
        "for domains in val_samples:\n",
        "    for domain in domains:\n",
        "        val_samples[frozenset([domain])] = {domain: val_samples[domains][domain]}\n",
        "    break\n",
        "# Create attr folder where we will save checkpoints\n",
        "(config.default_root_dir / \"gw\").mkdir(exist_ok=True)\n",
        "\n",
        "callbacks: list[Callback] = [\n",
        "        LogGWImagesCallback(\n",
        "            val_samples,\n",
        "            log_key=\"images/val\",\n",
        "            mode=\"val\",\n",
        "            every_n_epochs=config.logging.log_val_medias_every_n_epochs,\n",
        "            filter=config.logging.filter_images,\n",
        "            vocab=config.domain_modules.text.vocab_path,\n",
        "            merges=config.domain_modules.text.merges_path,\n",
        "        ),\n",
        "        LogGWImagesCallback(\n",
        "            train_samples,\n",
        "            log_key=\"images/train\",\n",
        "            mode=\"train\",\n",
        "            every_n_epochs=config.logging.log_train_medias_every_n_epochs,\n",
        "            filter=config.logging.filter_images,\n",
        "            vocab=config.domain_modules.text.vocab_path,\n",
        "            merges=config.domain_modules.text.merges_path,\n",
        "        ),\n",
        "        # Save the checkpoints\n",
        "        ModelCheckpoint(\n",
        "            dirpath=config.default_root_dir / \"gw\" / f\"version_{logger.version}\",\n",
        "            filename=\"{epoch}\",\n",
        "            monitor=\"val/loss\",\n",
        "            mode=\"min\",\n",
        "            save_last=\"link\",\n",
        "            save_top_k=1,\n",
        "        ),\n",
        "    ]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e3588e47",
      "metadata": {
        "id": "e3588e47"
      },
      "source": [
        "For the final model, let's save where the model is saved:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e257b74b",
      "metadata": {
        "id": "e257b74b"
      },
      "outputs": [],
      "source": [
        "gw_checkpoint = config.default_root_dir / \"gw\" / f\"version_{logger.version}\"\n",
        "print(gw_checkpoint)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5cd14d0b",
      "metadata": {
        "id": "5cd14d0b"
      },
      "source": [
        "Load tensorboard. You can select the version associated to the previous path. It will appear after the training is started."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bdeddfb5",
      "metadata": {
        "id": "bdeddfb5"
      },
      "outputs": [],
      "source": [
        "%load_ext tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "936b1b03",
      "metadata": {
        "id": "936b1b03"
      },
      "outputs": [],
      "source": [
        "%tensorboard --logdir \"./logs/gw\" --reload_interval 30 --reload_task 'auto'"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6739a7a1",
      "metadata": {
        "id": "6739a7a1"
      },
      "source": [
        "Let's train!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5dd80722",
      "metadata": {
        "id": "5dd80722"
      },
      "outputs": [],
      "source": [
        "trainer = Trainer(\n",
        "    logger=logger,\n",
        "    max_steps=config.training.max_steps,\n",
        "    default_root_dir=config.default_root_dir,\n",
        "    callbacks=callbacks,\n",
        "    precision=config.training.precision,\n",
        "    accelerator=config.training.accelerator,\n",
        "    devices=config.training.devices,\n",
        ")\n",
        "\n",
        "trainer.fit(global_workspace, data_module)\n",
        "trainer.validate(global_workspace, data_module, \"best\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "890fc24c",
      "metadata": {
        "id": "890fc24c"
      },
      "source": [
        "# Playing with the Global Workspace"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "36b91058",
      "metadata": {
        "id": "36b91058"
      },
      "source": [
        "The training of a Global Workspace can be long, especially with 3 modalities. So let's use a pre-trained checkpoint to play a bit with the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c5b0affa",
      "metadata": {
        "id": "c5b0affa"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4947da5c",
      "metadata": {
        "id": "4947da5c"
      },
      "source": [
        "The Global Workspace model we are going to use is a bit bigger than the one we started to train. It still relies on the same unimodal modules.\n",
        "\n",
        "So let's redefine the config parameters to fit with our model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1ff9a12b",
      "metadata": {
        "id": "1ff9a12b"
      },
      "source": [
        "We have to update `main.yaml` file\n",
        "\n",
        "```yaml\n",
        "global_workspace:\n",
        "    latent_dim: 32  \n",
        "    \n",
        "    loss_coefficients:\n",
        "        cycles: 1.0\n",
        "        contrastives: 0.1\n",
        "        demi_cycles: 1.0\n",
        "        translations: 1.0\n",
        "\n",
        "    encoders:\n",
        "        hidden_dim: 64\n",
        "        n_layers: 2\n",
        "\n",
        "    decoders:\n",
        "        hidden_dim:\n",
        "            v_latents: 128\n",
        "            t: 64\n",
        "            attr: 64\n",
        "        n_layers: 2\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0b183f1c",
      "metadata": {
        "id": "0b183f1c"
      },
      "outputs": [],
      "source": [
        "config = load_config(\"./config\", use_cli=False, load_files=[\"train_gw.yaml\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "179ec245",
      "metadata": {
        "id": "179ec245"
      },
      "source": [
        "We can now load pretrained unimodal modules and the defined Global Workspace encoders and decoders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c44c7074",
      "metadata": {
        "id": "c44c7074"
      },
      "outputs": [],
      "source": [
        "domain_modules, gw_encoders, gw_decoders = load_pretrained_domains(\n",
        "    config.domains,\n",
        "    config.global_workspace.latent_dim,\n",
        "    config.global_workspace.encoders.hidden_dim,\n",
        "    config.global_workspace.encoders.n_layers,\n",
        "    config.global_workspace.decoders.hidden_dim,\n",
        "    config.global_workspace.decoders.n_layers,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "14445890",
      "metadata": {
        "id": "14445890"
      },
      "source": [
        "Now let's load the Global Workspace from the checkpoint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "75783fa5",
      "metadata": {
        "id": "75783fa5"
      },
      "outputs": [],
      "source": [
        "checkpoint_path = Path(\"./checkpoints\")\n",
        "checkpoint = checkpoint_path / \"gw_dd835ybh.ckpt\" # \"gw_3_all.ckpt\"\n",
        "\n",
        "global_workspace = GlobalWorkspaceFusion.load_from_checkpoint(\n",
        "    checkpoint,\n",
        "    domain_mods=domain_modules,\n",
        "    gw_encoders=gw_encoders,\n",
        "    gw_decoders=gw_decoders,\n",
        ")\n",
        "global_workspace.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d1f595c6",
      "metadata": {
        "id": "d1f595c6"
      },
      "outputs": [],
      "source": [
        "domain_classes = get_default_domains([\"v_latents\", \"attr\", \"t\"])\n",
        "\n",
        "additional_transforms = {}\n",
        "additional_transforms[\"t\"] = [\n",
        "    TokenizeCaptions(\n",
        "        config.domain_modules.text.vocab_path,\n",
        "        config.domain_modules.text.merges_path,\n",
        "        config.domain_modules.text.seq_length,\n",
        "    )\n",
        "]\n",
        "\n",
        "data_module = SimpleShapesDataModule(\n",
        "        config.dataset.path,\n",
        "        domain_classes,\n",
        "        config.domain_proportions,\n",
        "        batch_size=config.training.batch_size,\n",
        "        num_workers=config.training.num_workers,\n",
        "        seed=config.seed,\n",
        "        ood_seed=config.ood_seed,\n",
        "        domain_args=config.domain_data_args,\n",
        "        additional_transforms=additional_transforms,\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cd6fc71f",
      "metadata": {
        "id": "cd6fc71f"
      },
      "source": [
        "And take some data from the dataset to play with"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ae0dfa3a",
      "metadata": {
        "id": "ae0dfa3a"
      },
      "outputs": [],
      "source": [
        "train_samples = data_module.get_samples(\"train\", 32)\n",
        "train_samples = batch_to_device(train_samples, device)\n",
        "print(train_samples.keys())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ba89456e",
      "metadata": {
        "id": "ba89456e"
      },
      "source": [
        "This group contains three modalities: vision, attributes and text all paired together"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d5d09285",
      "metadata": {
        "id": "d5d09285"
      },
      "outputs": [],
      "source": [
        "paired_samples = train_samples[frozenset([\"v_latents\", \"t\", \"attr\"])]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "55174ca5",
      "metadata": {
        "id": "55174ca5"
      },
      "source": [
        "### Images"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d9c6ae3e",
      "metadata": {
        "id": "d9c6ae3e"
      },
      "source": [
        "Let's first look at the selected images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d3c04646",
      "metadata": {
        "id": "d3c04646"
      },
      "outputs": [],
      "source": [
        "img_uni = paired_samples['v_latents']\n",
        "img = domain_modules['v_latents'].decode_images(img_uni)\n",
        "grid = make_grid(img, nrow=8, pad_value=1)\n",
        "plt.imshow(grid.permute(1, 2, 0).detach().cpu().numpy())\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e4fa669f",
      "metadata": {
        "id": "e4fa669f"
      },
      "source": [
        "### Attributes"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "45499bb6",
      "metadata": {
        "id": "45499bb6"
      },
      "source": [
        "Now let’s look at the selected attributes. As you can see, the raw attribute vectors are not easy to interpret directly.\n",
        "\n",
        "To make them more understandable, we will visualize the corresponding generated image whenever we refer to attributes in the rest of this tutorial. This makes it easier to grasp what the attribute representation actually means."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "21b6ceee",
      "metadata": {
        "id": "21b6ceee"
      },
      "outputs": [],
      "source": [
        "from simple_shapes_dataset.pre_process import tensor_to_attribute, UnnormalizeAttributes\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3cf2226f",
      "metadata": {
        "id": "3cf2226f"
      },
      "outputs": [],
      "source": [
        "unormalize = UnnormalizeAttributes()\n",
        "attr_uni = paired_samples['attr']\n",
        "attr = tensor_to_attribute(attr_uni)\n",
        "attr = unormalize(attr)\n",
        "\n",
        "data = {field: getattr(attr, field).tolist() for field in attr._fields[:-1]}\n",
        "df = pd.DataFrame(data)\n",
        "display(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c0d38d67",
      "metadata": {
        "id": "c0d38d67"
      },
      "source": [
        "### Text"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1ed8e875",
      "metadata": {
        "id": "1ed8e875"
      },
      "source": [
        "Finally, let’s take a look at the textual data. The sentences are stored as lists of tokens, but don’t worry, it’s easy to convert them back into natural language.\n",
        "We simply use the same tokenizer, but in reverse, to decode the tokens back into readable text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "96fec9fb",
      "metadata": {
        "id": "96fec9fb"
      },
      "outputs": [],
      "source": [
        "tokenizer = ByteLevelBPETokenizer(\n",
        "    config.domain_modules.text.vocab_path,\n",
        "    config.domain_modules.text.merges_path,\n",
        ")\n",
        "\n",
        "def get_text_samples(samples: dict[str, torch.Tensor]) -> list[str]:\n",
        "    \"\"\"\n",
        "    Decode the token sequence into a full sentence.\n",
        "    \"\"\"\n",
        "    return tokenizer.decode_batch(\n",
        "        samples[\"tokens\"].detach().cpu().tolist(),\n",
        "        skip_special_tokens=True,\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "197eeea0",
      "metadata": {
        "id": "197eeea0"
      },
      "outputs": [],
      "source": [
        "train_text_samples = get_text_samples(paired_samples[\"t\"])\n",
        "for k, text in enumerate(train_text_samples, 1):\n",
        "    print(k, text.replace(\"<pad>\", \"\"))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c77f125a",
      "metadata": {
        "id": "c77f125a"
      },
      "source": [
        "## Translations"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "374ad518",
      "metadata": {
        "id": "374ad518"
      },
      "source": [
        "Now that we are familiar with the data, let's use the global workspace to do some translations."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fd21c49a",
      "metadata": {
        "id": "fd21c49a"
      },
      "source": [
        "### Translation vision to text"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8f1e4d2f",
      "metadata": {
        "id": "8f1e4d2f"
      },
      "source": [
        "Let's begin with vision to text translation. This task can be called image captioning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d589ba97",
      "metadata": {
        "id": "d589ba97"
      },
      "outputs": [],
      "source": [
        "import matplotlib.gridspec as gridspec\n",
        "import textwrap"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ec4cf51e",
      "metadata": {
        "id": "ec4cf51e"
      },
      "source": [
        "We first encode the raw data into the unimodal latent representations with encode_domain:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "40cf2aaf",
      "metadata": {
        "id": "40cf2aaf"
      },
      "outputs": [],
      "source": [
        "latent_img_uni = global_workspace.encode_domain(paired_samples[\"v_latents\"], \"v_latents\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6a3be5ea",
      "metadata": {
        "id": "6a3be5ea"
      },
      "source": [
        "Then, we encode the unimodal visual representation into the GW reprensentation. First we encode the reprensentations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1278e531",
      "metadata": {
        "id": "1278e531"
      },
      "outputs": [],
      "source": [
        "gw_latent_v = global_workspace.gw_mod.encode({\"v_latents\": latent_img_uni})"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3b1cc7d7",
      "metadata": {
        "id": "3b1cc7d7"
      },
      "source": [
        "Then we fuse the latents together. As we have only one input (the latent images), we put all the weights on the visual modality"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0b364233",
      "metadata": {
        "id": "0b364233"
      },
      "outputs": [],
      "source": [
        "gw_latent_v_fused = global_workspace.gw_mod.fuse(gw_latent_v, {\"v_latents\": torch.ones(gw_latent_v['v_latents'].size(0)).to(device)})"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d48514fd",
      "metadata": {
        "id": "d48514fd"
      },
      "source": [
        "Finally, we decode the Global Workspace representation:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "838f1fa2",
      "metadata": {
        "id": "838f1fa2"
      },
      "outputs": [],
      "source": [
        "decoded_latent_uni = global_workspace.gw_mod.decode(gw_latent_v_fused)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dc19e3b7",
      "metadata": {
        "id": "dc19e3b7"
      },
      "source": [
        "Now let's visualize what we obtained.\n",
        "\n",
        "The Figure represent each generated caption with the ground truth image bellow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2f96d15e",
      "metadata": {
        "id": "2f96d15e"
      },
      "outputs": [],
      "source": [
        "decoded_texts = global_workspace.decode_domain(decoded_latent_uni[\"t\"], \"t\")\n",
        "reconstructed_text = get_text_samples(decoded_texts)\n",
        "\n",
        "cols = 4\n",
        "rows = 8\n",
        "fig = plt.figure(figsize=(16, 24))\n",
        "gs = gridspec.GridSpec(rows, cols, wspace=0.3, hspace=1.0)\n",
        "\n",
        "for i in range(32):\n",
        "    ax = fig.add_subplot(gs[i])\n",
        "    ax.imshow(img[i].permute(1, 2, 0).detach().cpu().numpy())\n",
        "    wrapped_caption = \"\\n\".join(textwrap.wrap(reconstructed_text[i].replace(\"<pad>\", \"\"), width=40))\n",
        "    ax.set_title(wrapped_caption, fontsize=10)\n",
        "    ax.axis('off')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "319e1b8a",
      "metadata": {
        "id": "319e1b8a"
      },
      "source": [
        "### Translation text to vision"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e8c575ce",
      "metadata": {
        "id": "e8c575ce"
      },
      "source": [
        "We can try the other way aroung, translating text into images.\n",
        "\n",
        "We follow the same process as before.\n",
        "\n",
        "First we encode the text into unimodal latent representation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "71d9ea0c",
      "metadata": {
        "id": "71d9ea0c"
      },
      "outputs": [],
      "source": [
        "latent_txt_uni = global_workspace.encode_domain(paired_samples[\"t\"], \"t\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "07c864e1",
      "metadata": {
        "id": "07c864e1"
      },
      "source": [
        "Then we encode the unimoal representation inside the Global Workspace"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fea7d3be",
      "metadata": {
        "id": "fea7d3be"
      },
      "outputs": [],
      "source": [
        "gw_latent_t = global_workspace.gw_mod.encode({\"t\": latent_txt_uni})"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6ce4c8d2",
      "metadata": {
        "id": "6ce4c8d2"
      },
      "source": [
        "Before fusing the representations. As we only have text encoded this time we put all the weights on the text modality"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6fab46cb",
      "metadata": {
        "id": "6fab46cb"
      },
      "outputs": [],
      "source": [
        "gw_latent_t_fused = global_workspace.gw_mod.fuse(gw_latent_t, {\"t\": torch.ones(gw_latent_t['t'].size(0)).to(device)})"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8b6bfc48",
      "metadata": {
        "id": "8b6bfc48"
      },
      "source": [
        "And we finally decode it from the Global Workspace to unimodal representations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0fe2929d",
      "metadata": {
        "id": "0fe2929d"
      },
      "outputs": [],
      "source": [
        "decoded_latent_uni = global_workspace.gw_mod.decode(gw_latent_t_fused)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9fbaa32b",
      "metadata": {
        "id": "9fbaa32b"
      },
      "source": [
        "Now let's visualize what we obtained.\n",
        "\n",
        "The Figure represent each ground truth caption with the predicted image bellow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1d5e9bd7",
      "metadata": {
        "id": "1d5e9bd7"
      },
      "outputs": [],
      "source": [
        "decoded_images = domain_modules['v_latents'].decode_images(decoded_latent_uni['v_latents'])\n",
        "text_input = get_text_samples(paired_samples[\"t\"])\n",
        "\n",
        "cols = 4\n",
        "rows = 8\n",
        "fig = plt.figure(figsize=(16, 24))\n",
        "gs = gridspec.GridSpec(rows, cols, wspace=0.3, hspace=1.0)\n",
        "\n",
        "for i in range(32):\n",
        "    ax = fig.add_subplot(gs[i])\n",
        "    ax.imshow(decoded_images[i].permute(1, 2, 0).detach().cpu().numpy())\n",
        "    wrapped_caption = \"\\n\".join(textwrap.wrap(train_text_samples[i].replace(\"<pad>\", \"\"), width=40))\n",
        "    ax.set_title(wrapped_caption, fontsize=10)\n",
        "    ax.axis('off')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "efd506d6",
      "metadata": {
        "id": "efd506d6"
      },
      "source": [
        "### Translation attributes to any"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e6ab58e6",
      "metadata": {
        "id": "e6ab58e6"
      },
      "outputs": [],
      "source": [
        "latent_attr_uni = global_workspace.encode_domain(paired_samples[\"attr\"], \"attr\")\n",
        "gw_latent_attr = global_workspace.gw_mod.encode({\"attr\": latent_attr_uni})\n",
        "gw_latent_attr_fused = global_workspace.gw_mod.fuse(gw_latent_attr, {\"attr\": torch.ones(gw_latent_attr['attr'].size(0)).to(device)})\n",
        "decoded_latent_uni = global_workspace.gw_mod.decode(gw_latent_attr_fused)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4d6cca5e",
      "metadata": {
        "id": "4d6cca5e"
      },
      "source": [
        "## Fusion Mechanism"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4a1e0bd6",
      "metadata": {
        "id": "4a1e0bd6"
      },
      "source": [
        "Now that we’re familiar with translation, let’s explore the impact of the fusion mechanism in more detail.\n",
        "\n",
        "In this section, we’ll generate two different inputs that vary along one or more dimensions (shape class, location, size, rotation, or color) across two different modalities (e.g., vision and attributes).\n",
        "These two inputs will then be fused inside the Global Workspace, using a gradient of attention weights that shifts gradually from giving full attention to the first modality to giving full attention to the second.\n",
        "\n",
        "To better understand how the model handles conflicting or complementary information, we’ll visualize this process by displaying a sequence of images, each corresponding to a different attention coefficient. This allows us to observe how the Global Workspace integrates multimodal inputs under varying fusion conditions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "07fffa53",
      "metadata": {
        "id": "07fffa53"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torchvision\n",
        "from PIL import Image\n",
        "from torchvision.utils import save_image\n",
        "\n",
        "from simple_shapes_dataset.domain import Attribute\n",
        "from simple_shapes_dataset.cli.utils import generate_image, generate_class, generate_scale, generate_location, generate_rotation, generate_color\n",
        "from simple_shapes_dataset.pre_process import attribute_to_tensor, tensor_to_attribute, NormalizeAttributes, UnnormalizeAttributes"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1b824e6c",
      "metadata": {
        "id": "1b824e6c"
      },
      "source": [
        "Let's first generate the two different data we are going to pass to the Global Workspace.\n",
        "\n",
        "In this case we are going to fuse attributes and vision data.\n",
        "For this we generate random attributes that will serve either as attributes modality or to generate an image."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6cd87464",
      "metadata": {
        "id": "6cd87464"
      },
      "outputs": [],
      "source": [
        "n_samples = 1\n",
        "min_scale = 7\n",
        "max_scale = 14\n",
        "imsize = 32\n",
        "min_lightness = 0\n",
        "max_lightness = 256\n",
        "dpi = 1\n",
        "\n",
        "classes = generate_class(n_samples)\n",
        "sizes = generate_scale(n_samples, min_scale, max_scale)\n",
        "locations = generate_location(n_samples, max_scale, imsize)\n",
        "rotation = generate_rotation(n_samples)\n",
        "colors_rgb, colors_hls = generate_color(n_samples, min_lightness, max_lightness)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7bd52013",
      "metadata": {
        "id": "7bd52013"
      },
      "source": [
        "Now let's specify which attributes we want to change between vision and attributes. In this case we are going to change the size and color of the shape."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5851f969",
      "metadata": {
        "id": "5851f969"
      },
      "outputs": [],
      "source": [
        "colors_rgb_v = np.array([[243, 57, 82]])\n",
        "sizes_v  = np.array([7.])\n",
        "\n",
        "colors_rgb_attr = np.array([[57, 88, 243]])\n",
        "sizes_attr = np.array([14.])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dcf0f892",
      "metadata": {
        "id": "dcf0f892"
      },
      "source": [
        "Let's display the input image for the Global Workspace"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ac08c1b7",
      "metadata": {
        "id": "ac08c1b7"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(figsize=(imsize / dpi, imsize / dpi), dpi=dpi)\n",
        "ax = cast(plt.Axes, ax)\n",
        "generate_image(ax, classes[0], locations[0], sizes_v[0], rotation[0], colors_rgb_v[0], imsize)\n",
        "ax.set_facecolor(\"black\")\n",
        "plt.tight_layout(pad=0)\n",
        "plt.savefig('gt_v.png', dpi=dpi, format=\"png\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2e680ba8",
      "metadata": {
        "id": "2e680ba8"
      },
      "source": [
        "Let's display the image associated to the input attributes for the Global Workspace"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ebee922b",
      "metadata": {
        "id": "ebee922b"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(figsize=(imsize / dpi, imsize / dpi), dpi=dpi)\n",
        "ax = cast(plt.Axes, ax)\n",
        "generate_image(ax, classes[0], locations[0], sizes_attr[0], rotation[0], colors_rgb_attr[0], imsize)\n",
        "ax.set_facecolor(\"black\")\n",
        "plt.tight_layout(pad=0)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "93142a02",
      "metadata": {
        "id": "93142a02"
      },
      "source": [
        "Now let's encode the raw modalities into unimodal latent vectors and get the expected data format for the Global Workspace."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7ba4c10d",
      "metadata": {
        "id": "7ba4c10d"
      },
      "outputs": [],
      "source": [
        "path = 'gt_v.png'\n",
        "with Image.open(path) as image:\n",
        "    img = image.convert(\"RGB\")\n",
        "\n",
        "tensor_img = torchvision.transforms.functional.pil_to_tensor(img).to(global_workspace.device).to(dtype=torch.float32) / 255.\n",
        "\n",
        "\n",
        "norm_attr = NormalizeAttributes()\n",
        "unorm_attr = UnnormalizeAttributes()\n",
        "\n",
        "attr = torch.FloatTensor(np.concatenate([classes, locations[0], sizes_attr, rotation, colors_rgb_attr[0]])).to(device)\n",
        "\n",
        "attr = Attribute(\n",
        "    category = attr[0].to(torch.long),\n",
        "    x = attr[1],\n",
        "    y = attr[2],\n",
        "    size = attr[3],\n",
        "    rotation = attr[4],\n",
        "    color_r = attr[5] / 255,\n",
        "    color_g = attr[6] / 255,\n",
        "    color_b = attr[7] / 255,\n",
        "    unpaired = None\n",
        ")\n",
        "\n",
        "attr = norm_attr(attr)\n",
        "attr_tensor = attribute_to_tensor(attr)\n",
        "\n",
        "uni_attr = domain_modules['attr'].encode(attr_tensor)\n",
        "uni_attr = uni_attr.unsqueeze(0)\n",
        "\n",
        "pred_attr = domain_modules['attr'].decode(uni_attr)\n",
        "pred_attr = tensor_to_attribute(pred_attr)\n",
        "pred_attr = unorm_attr(pred_attr)\n",
        "\n",
        "uni_v = domain_modules['v_latents'].visual_module.vae.encoder(tensor_img[None,:])[0]\n",
        "\n",
        "data = {frozenset({'v_latents', 'attr'}): {'v_latents': uni_v, 'attr': uni_attr}}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6d8ce0e2",
      "metadata": {
        "id": "6d8ce0e2"
      },
      "source": [
        "We are going to fuse the image and attributes with weights between 0 (all attention on image) to 1 (all attention on attributes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5a3d9c7c",
      "metadata": {
        "id": "5a3d9c7c"
      },
      "outputs": [],
      "source": [
        "images = []\n",
        "for attention in np.linspace(0,1,11):\n",
        "    selection = {\"attr\": torch.ones(1).to(device) * attention, \"v_latents\": torch.ones(1).to(device) * (1 - attention)}\n",
        "    gw_latent = global_workspace.gw_mod.encode(data[frozenset({'v_latents', 'attr'})])\n",
        "    gw_latent_fused = global_workspace.gw_mod.fuse(gw_latent, selection)\n",
        "\n",
        "    pred_attr_uni = global_workspace.gw_mod.decode(gw_latent_fused)['attr']\n",
        "    pred_v_uni = global_workspace.gw_mod.decode(gw_latent_fused)\n",
        "\n",
        "    decoded = global_workspace.gw_mod.decode(gw_latent_fused)\n",
        "\n",
        "    attr_pred = decoded[\"attr\"]\n",
        "    v_pred = decoded[\"v_latents\"]\n",
        "\n",
        "    new_img = domain_modules['v_latents'].decode_images(v_pred)\n",
        "    images.append(new_img[0])\n",
        "\n",
        "grid = make_grid(images, nrow=11, pad_value=1)\n",
        "plt.imshow(grid.permute(1, 2, 0).detach().cpu().numpy())\n",
        "plt.axis('off')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "18164be4",
      "metadata": {
        "id": "18164be4"
      },
      "source": [
        "## Play by yourself"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d9dd9abb",
      "metadata": {
        "id": "d9dd9abb"
      },
      "outputs": [],
      "source": [
        "%pip install ipywidgets\n",
        "%pip install ipympl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fc5ee08a",
      "metadata": {
        "id": "fc5ee08a"
      },
      "outputs": [],
      "source": [
        "import io\n",
        "import math\n",
        "\n",
        "import ipywidgets as widgets\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from ipywidgets import interact, interact_manual\n",
        "from PIL import Image\n",
        "from shimmer_ssd.logging import attribute_image_grid\n",
        "from torch.nn.functional import one_hot\n",
        "from torchvision.transforms.functional import to_tensor\n",
        "\n",
        "from simple_shapes_dataset.cli import generate_image\n",
        "%matplotlib widget"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0070188a",
      "metadata": {
        "id": "0070188a"
      },
      "source": [
        "### Generate images and text from attributes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3f38ee66",
      "metadata": {
        "id": "3f38ee66"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "global_workspace.to(device)\n",
        "\n",
        "cat2idx = {\"Diamond\": 0, \"Egg\": 1, \"Triangle\": 2}\n",
        "\n",
        "def get_image(cat, x, y, size, rot, color_r, color_g, color_b):\n",
        "    fig, ax = plt.subplots(figsize=(32, 32), dpi=1)\n",
        "    # The dataset generatoion tool has function to generate a matplotlib shape\n",
        "    # from the attributes.\n",
        "    generate_image(\n",
        "        ax,\n",
        "        cat2idx[cat],\n",
        "        [int(x * 18 + 7), int(y * 18 + 7)],\n",
        "        size * 7 + 7,\n",
        "        rot * 2 * math.pi,\n",
        "        np.array([color_r * 255, color_g * 255, color_b * 255]),\n",
        "        imsize=32,\n",
        "    )\n",
        "    ax.set_facecolor(\"black\")\n",
        "    plt.tight_layout(pad=0)\n",
        "    # Return this as a PIL Image.\n",
        "    # This is to have the same dpi as saved images\n",
        "    # otherwise matplotlib will render this in very high quality\n",
        "    buf = io.BytesIO()\n",
        "    fig.savefig(buf)\n",
        "    buf.seek(0)\n",
        "    image = Image.open(buf)\n",
        "    plt.close(fig)\n",
        "    return image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9fe101b7",
      "metadata": {
        "id": "9fe101b7"
      },
      "outputs": [],
      "source": [
        "tokenizer = ByteLevelBPETokenizer(\n",
        "    config.domain_modules.text.vocab_path,\n",
        "    config.domain_modules.text.merges_path,\n",
        ")\n",
        "\n",
        "def get_text_samples(samples: dict[str, torch.Tensor]) -> list[str]:\n",
        "    \"\"\"\n",
        "    Decode the token sequence into a full sentence.\n",
        "    \"\"\"\n",
        "    return tokenizer.decode_batch(\n",
        "        samples[\"tokens\"].detach().cpu().tolist(),\n",
        "        skip_special_tokens=True,\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3b03a93d",
      "metadata": {
        "id": "3b03a93d"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "global_workspace.to(device)\n",
        "\n",
        "cat2idx = {\"Diamond\": 0, \"Egg\": 1, \"Triangle\": 2}\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
        "img_axes = [axes[0], axes[1]]\n",
        "\n",
        "titles = [\"Original image from attributes\", \"Translated image from attributes\"]\n",
        "for ax, title in zip(img_axes, titles):\n",
        "    ax.set_title(title, fontsize=12, pad=10)\n",
        "    ax.set_xticks([])\n",
        "    ax.set_yticks([])\n",
        "    ax.set_facecolor(\"black\")\n",
        "\n",
        "text_box = fig.text(0.5, 0.05, \"\", ha='center', va='center', fontsize=12, wrap=True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.subplots_adjust(top=0.85, bottom=0.2)\n",
        "\n",
        "@interact(\n",
        "    cat=[\"Triangle\", \"Egg\", \"Diamond\"],\n",
        "    x=(0, 1, 0.1),\n",
        "    y=(0, 1, 0.1),\n",
        "    rot=(0, 1, 0.1),\n",
        "    size=(0, 1, 0.1),\n",
        "    color_r=(0, 1, 0.1),\n",
        "    color_g=(0, 1, 0.1),\n",
        "    color_b=(0, 1, 0.1),\n",
        ")\n",
        "def update(\n",
        "    cat: str = \"Triangle\",\n",
        "    x: float = 0.5,\n",
        "    y: float = 0.5,\n",
        "    rot: float = 0.5,\n",
        "    size: float = 0.5,\n",
        "    color_r: float = 1,\n",
        "    color_g: float = 0,\n",
        "    color_b: float = 0,\n",
        "):\n",
        "    image = get_image(cat, x, y, size, rot, color_r, color_g, color_b)\n",
        "    image_tensor = to_tensor(image)[:3].unsqueeze(0).to(device)\n",
        "    image_latent = domain_modules['v_latents'].visual_module.encode(image_tensor)\n",
        "\n",
        "    attr_gw_latent = global_workspace.gw_mod.encode({\n",
        "        \"v_latents\": global_workspace.encode_domain(image_latent, \"v_latents\")\n",
        "    })\n",
        "\n",
        "    gw_latent = global_workspace.gw_mod.fuse(\n",
        "        attr_gw_latent,\n",
        "        {\"v_latents\": torch.ones(attr_gw_latent[\"v_latents\"].size(0)).to(device)}\n",
        "    )\n",
        "\n",
        "    decoded_latents = global_workspace.gw_mod.decode(gw_latent)\n",
        "    decoded_texts = global_workspace.decode_domain(decoded_latents[\"t\"], \"t\")\n",
        "    reconstructed_text = get_text_samples(decoded_texts)[0].replace(\"<pad>\", \"\").strip()\n",
        "\n",
        "    decoded_img_tensor = global_workspace.domain_mods[\"v_latents\"].decode_images(decoded_latents[\"v_latents\"])[0]\n",
        "    decoded_img = decoded_img_tensor.permute(1, 2, 0).detach().cpu().numpy()\n",
        "\n",
        "    for ax in img_axes:\n",
        "        for im in ax.get_images():\n",
        "            im.remove()\n",
        "\n",
        "    img_axes[0].imshow(image)\n",
        "    img_axes[1].imshow(decoded_img)\n",
        "\n",
        "    text_box.set_text(reconstructed_text)\n",
        "\n",
        "    fig.canvas.draw_idle()\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}